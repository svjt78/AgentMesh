# Context Engineering Observability Features

This document details the three major observability features in AgentMesh's Context Engineering system: **Context Engineering Logs**, **Token Analytics**, and **Explainability**.

---

## Table of Contents

1. [Overview](#overview)
2. [Context Engineering Log](#context-engineering-log)
3. [Token Analytics](#token-analytics)
4. [Explainability (Evidence Maps)](#explainability-evidence-maps)
5. [Data Flow Architecture](#data-flow-architecture)
6. [API Reference](#api-reference)
7. [Frontend Components](#frontend-components)
8. [Usage Examples](#usage-examples)
9. [Configuration](#configuration)

---

## Overview

AgentMesh provides three complementary observability features for understanding and optimizing context engineering:

| Feature | Purpose | Primary Users | Key Metrics |
|---------|---------|---------------|-------------|
| **Context Engineering Log** | Debug context compilation pipeline | Developers, DevOps | Processor execution times, token counts, lineage |
| **Token Analytics** | Optimize token usage and costs | Product Managers, Engineers | Token distribution, truncation, compaction ratios |
| **Explainability** | Understand AI decisions | Business Users, Regulators | Evidence weights, agent chain, confidence scores |

**Storage Locations:**
```
storage/
├── sessions/{session_id}.jsonl                      # All events
├── sessions/{session_id}_context_lineage.jsonl      # Context engineering log
└── artifacts/{session_id}_evidence.json             # Explainability evidence map
```

**Frontend Access:**
- Context Engineering Log → Token Analytics tab → Context Lineage Tree visualization
- Token Analytics → Token Analytics tab → Multiple charts
- Explainability → Explainability tab → 6-section evidence display

---

## Context Engineering Log

### What It Is

The **Context Engineering Log** provides a complete audit trail of every context compilation event, tracking all 7 processors in the pipeline with detailed execution metrics.

**File Format:** JSONL (JSON Lines)
**Location:** `storage/sessions/{session_id}_context_lineage.jsonl`
**Purpose:** Debug context compilation, understand token optimization decisions, trace context transformations

### Generated By

#### Primary: ContextLineageTracker
**File:** [`backend/orchestrator/app/services/context_lineage_tracker.py`](backend/orchestrator/app/services/context_lineage_tracker.py)

**Role:**
- Centralized logging service for all context compilation events
- Tracks processor execution metrics
- Records token counts and transformations
- Logs memory retrieval and artifact access

**Key Methods:**
```python
class ContextLineageTracker:
    async def log_compilation(
        session_id: str,
        agent_id: str,
        processors_executed: List[ProcessorResult],
        token_counts: Dict[str, int],
        truncation_details: Optional[Dict],
        memories_retrieved: List[str],
        artifacts_accessed: List[str]
    ) -> None:
        """Log complete context compilation event"""

    async def log_processor_execution(
        session_id: str,
        processor_id: str,
        execution_time_ms: float,
        success: bool,
        modifications: Dict
    ) -> None:
        """Log individual processor execution"""

    async def get_compilation_stats(
        session_id: str
    ) -> Dict:
        """Retrieve aggregated compilation statistics"""
```

#### Supporting: ContextProcessorPipeline
**File:** [`backend/orchestrator/app/services/context_processor_pipeline.py`](backend/orchestrator/app/services/context_processor_pipeline.py)

**Role:**
- Executes 7 processors in configured order
- Collects execution metrics from each processor
- Invokes ContextLineageTracker to log results

**Pipeline Execution Flow:**
```python
async def execute(
    context: List[Dict],
    agent_id: str,
    session_id: str
) -> ProcessorPipelineResult:
    processor_results = []

    # Execute each enabled processor in order
    for processor_config in self.pipeline:
        if not processor_config["enabled"]:
            continue

        processor = self._get_processor(processor_config["processor_id"])

        # Execute processor
        result = await processor.process(context, agent_id, session_id)
        processor_results.append(result)

        # Log execution
        await self.lineage_tracker.log_processor_execution(
            session_id=session_id,
            processor_id=processor_config["processor_id"],
            execution_time_ms=result.execution_time_ms,
            success=result.success,
            modifications=result.modifications_made
        )

        # Update context for next processor
        context = result.context

    # Log complete compilation
    await self.lineage_tracker.log_compilation(
        session_id=session_id,
        agent_id=agent_id,
        processors_executed=processor_results,
        token_counts=self._count_tokens(context),
        truncation_details=self._get_truncation_details(processor_results),
        memories_retrieved=self._get_memories(processor_results),
        artifacts_accessed=self._get_artifacts(processor_results)
    )

    return ProcessorPipelineResult(context=context, metrics=processor_results)
```

#### Supporting: GovernanceAuditor
**File:** [`backend/orchestrator/app/services/governance_auditor.py`](backend/orchestrator/app/services/governance_auditor.py)

**Role:**
- Logs governance decisions related to context engineering
- Tracks filtering, masking, and access control
- Provides audit trail for compliance

### Log Entry Schema

**Compilation Event:**
```json
{
  "event_type": "context_compiled",
  "timestamp": "2024-01-05T10:30:15.234Z",
  "session_id": "sess_abc123",
  "agent_id": "fraud_agent",
  "data": {
    "compilation_id": "comp_xyz789",
    "processors_executed": [
      {
        "processor_id": "content_selector",
        "order": 1,
        "execution_time_ms": 12.5,
        "success": true,
        "modifications_made": {
          "events_filtered": 25,
          "events_retained": 75
        }
      },
      {
        "processor_id": "compaction_checker",
        "order": 2,
        "execution_time_ms": 8.3,
        "success": true,
        "modifications_made": {
          "compaction_triggered": false,
          "token_count": 5200
        }
      },
      {
        "processor_id": "memory_retriever",
        "order": 3,
        "execution_time_ms": 45.7,
        "success": true,
        "modifications_made": {
          "memories_retrieved": 3,
          "retrieval_mode": "proactive"
        }
      },
      {
        "processor_id": "artifact_resolver",
        "order": 4,
        "execution_time_ms": 23.1,
        "success": true,
        "modifications_made": {
          "artifacts_resolved": 2,
          "artifacts_kept_as_handles": 1
        }
      },
      {
        "processor_id": "transformer",
        "order": 5,
        "execution_time_ms": 18.9,
        "success": true,
        "modifications_made": {
          "events_transformed": 78
        }
      },
      {
        "processor_id": "token_budget_enforcer",
        "order": 6,
        "execution_time_ms": 15.2,
        "success": true,
        "modifications_made": {
          "truncation_applied": true,
          "tokens_before": 9500,
          "tokens_after": 8000,
          "items_truncated": ["older_outputs"]
        }
      },
      {
        "processor_id": "injector",
        "order": 7,
        "execution_time_ms": 5.4,
        "success": true,
        "modifications_made": {
          "system_instructions_added": true
        }
      }
    ],
    "token_counts": {
      "input_context": 2400,
      "prior_outputs": 4000,
      "tool_observations": 1600,
      "total": 8000
    },
    "truncation_details": {
      "truncation_applied": true,
      "original_tokens": 9500,
      "final_tokens": 8000,
      "budget_limit": 8000,
      "truncated_sections": ["older_agent_outputs"]
    },
    "memories_retrieved": [
      "mem_fraud_pattern_auto_claim",
      "mem_customer_pref_fast_approval",
      "mem_recent_fraud_spike"
    ],
    "artifacts_accessed": [
      "artifact://sess_abc123_evidence/v1",
      "artifact://policy_database/v5"
    ],
    "total_pipeline_time_ms": 129.1
  }
}
```

**Processor Execution Event:**
```json
{
  "event_type": "processor_executed",
  "timestamp": "2024-01-05T10:30:15.250Z",
  "session_id": "sess_abc123",
  "data": {
    "processor_id": "token_budget_enforcer",
    "agent_id": "fraud_agent",
    "execution_time_ms": 15.2,
    "success": true,
    "input_token_count": 9500,
    "output_token_count": 8000,
    "modifications_made": {
      "truncation_applied": true,
      "tokens_removed": 1500,
      "budget_allocation": {
        "input_budget": 2400,
        "outputs_budget": 4000,
        "observations_budget": 1600
      }
    }
  }
}
```

### Use Cases

1. **Performance Debugging:**
   - Identify slow processors
   - Optimize pipeline execution order
   - Track processor execution trends

2. **Context Optimization:**
   - Understand why truncation occurred
   - Verify memory retrieval working correctly
   - Validate artifact resolution

3. **Compliance Auditing:**
   - Prove governance policies enforced
   - Show data access patterns
   - Demonstrate context filtering

4. **Cost Analysis:**
   - Track token usage over time
   - Identify optimization opportunities
   - Correlate token usage with decisions

---

## Token Analytics

### What It Is

**Token Analytics** provides comprehensive metrics about token usage, budget enforcement, and optimization across the entire session lifecycle.

**Data Source:** Session JSONL events + Context lineage JSONL
**API Endpoint:** `GET /sessions/{session_id}/context-stats`
**Purpose:** Monitor token costs, optimize context compilation, prevent overages

### Generated By

Token analytics data is aggregated from multiple services:

#### 1. ContextProcessorPipeline
**File:** [`backend/orchestrator/app/services/context_processor_pipeline.py`](backend/orchestrator/app/services/context_processor_pipeline.py)

**Tracks:**
- Total tokens per compilation
- Processor execution metrics
- Pipeline performance

**Events Logged:**
```json
{
  "event_type": "context_compiled",
  "data": {
    "token_counts": {
      "input_context": 2400,
      "prior_outputs": 4000,
      "tool_observations": 1600,
      "total": 8000
    },
    "total_pipeline_time_ms": 129.1
  }
}
```

#### 2. TokenBudgetEnforcer (Processor)
**File:** [`backend/orchestrator/app/services/processors/token_budget_enforcer.py`](backend/orchestrator/app/services/processors/token_budget_enforcer.py)

**Tracks:**
- Budget limits enforced
- Truncation events and details
- Budget allocation (30% input, 50% outputs, 20% observations)

**Configuration:** `registries/context_strategies.json`
```json
{
  "token_budgets": {
    "default_budget": 8000,
    "budget_allocation": {
      "input_context": 0.30,
      "prior_outputs": 0.50,
      "tool_observations": 0.20
    }
  }
}
```

**Events Logged:**
```json
{
  "event_type": "context_truncated",
  "timestamp": "2024-01-05T10:30:15.265Z",
  "session_id": "sess_abc123",
  "data": {
    "agent_id": "fraud_agent",
    "original_tokens": 9500,
    "final_tokens": 8000,
    "budget_limit": 8000,
    "truncation_strategy": "prioritize_recent",
    "truncated_items": {
      "older_agent_outputs": 1200,
      "tool_observations": 300
    },
    "budget_allocation_used": {
      "input_context": 2400,
      "prior_outputs": 4000,
      "tool_observations": 1600
    }
  }
}
```

#### 3. CompactionManager
**File:** [`backend/orchestrator/app/services/compaction_manager.py`](backend/orchestrator/app/services/compaction_manager.py)

**Tracks:**
- Session compaction triggers
- Token reduction from summarization
- Compression ratios

**Configuration:** `registries/context_strategies.json`
```json
{
  "compaction": {
    "enabled": true,
    "token_threshold": 8000,
    "event_count_threshold": 100,
    "compaction_strategy": "summarize_with_llm"
  }
}
```

**Events Logged:**
```json
{
  "event_type": "compaction_triggered",
  "timestamp": "2024-01-05T10:25:00.000Z",
  "session_id": "sess_abc123",
  "data": {
    "trigger_reason": "token_threshold_exceeded",
    "current_tokens": 8500,
    "threshold": 8000
  }
}
```

```json
{
  "event_type": "compaction_completed",
  "timestamp": "2024-01-05T10:25:30.000Z",
  "session_id": "sess_abc123",
  "data": {
    "events_before": 150,
    "events_after": 50,
    "tokens_before": 15000,
    "tokens_after": 5000,
    "compression_ratio": 0.33,
    "compaction_summary": "Summarized intake, coverage, and fraud analysis...",
    "archived_to": "storage/compactions/sess_abc123_20240105.json"
  }
}
```

#### 4. LLMClient
**File:** [`backend/orchestrator/app/services/llm_client.py`](backend/orchestrator/app/services/llm_client.py)

**Tracks:**
- Actual tokens used per LLM call
- Token costs per provider
- Request/response token split

**Events Logged:**
```json
{
  "event_type": "llm_call_completed",
  "data": {
    "model": "gpt-4-turbo",
    "prompt_tokens": 7950,
    "completion_tokens": 320,
    "total_tokens": 8270,
    "estimated_cost_usd": 0.0912
  }
}
```

### Analytics Schema

**API Response:** `GET /sessions/{session_id}/context-stats`

```json
{
  "session_id": "sess_abc123",
  "analytics": {
    "compilation_stats": {
      "total_compilations": 12,
      "avg_compilation_time_ms": 125.5,
      "total_tokens_processed": 96000,
      "avg_tokens_per_compilation": 8000
    },
    "token_usage": {
      "total_input_tokens": 28800,
      "total_output_tokens": 48000,
      "total_observation_tokens": 19200,
      "total_tokens": 96000,
      "avg_budget_utilization": 0.95
    },
    "truncation_events": {
      "total_truncations": 5,
      "total_tokens_truncated": 7500,
      "avg_tokens_truncated": 1500,
      "truncation_rate": 0.42
    },
    "compaction_events": {
      "total_compactions": 2,
      "total_tokens_saved": 10000,
      "avg_compression_ratio": 0.35
    },
    "processor_performance": {
      "content_selector": {
        "avg_execution_time_ms": 12.3,
        "total_executions": 12,
        "success_rate": 1.0
      },
      "token_budget_enforcer": {
        "avg_execution_time_ms": 15.8,
        "total_executions": 12,
        "success_rate": 1.0,
        "truncations_applied": 5
      }
      // ... other processors
    },
    "cost_estimates": {
      "total_estimated_cost_usd": 1.25,
      "cost_by_agent": {
        "fraud_agent": 0.45,
        "coverage_agent": 0.30,
        "recommendation_agent": 0.50
      }
    }
  },
  "timeline": [
    {
      "timestamp": "2024-01-05T10:30:15.234Z",
      "agent_id": "fraud_agent",
      "tokens_used": 8000,
      "truncation_applied": true,
      "compaction_applied": false
    }
    // ... more timeline entries
  ]
}
```

### Use Cases

1. **Cost Optimization:**
   - Identify expensive agents
   - Track token usage trends
   - Optimize budget allocations

2. **Performance Monitoring:**
   - Monitor compilation times
   - Identify bottlenecks
   - Track processor efficiency

3. **Capacity Planning:**
   - Predict token usage
   - Plan for scaling
   - Set budget limits

4. **Debugging:**
   - Understand why truncation occurred
   - Verify compaction working
   - Track token distribution

---

## Explainability (Evidence Maps)

### What It Is

**Explainability** provides a structured, evidence-based explanation of AI decisions with complete transparency into the reasoning process.

**File Format:** JSON
**Location:** `storage/artifacts/{session_id}_evidence.json`
**API Endpoint:** `GET /sessions/{session_id}/evidence`
**Purpose:** Business justification, regulatory compliance, customer communication

### Generated By

#### Primary: OrchestratorRunner
**File:** [`backend/orchestrator/app/services/orchestrator_runner.py`](backend/orchestrator/app/services/orchestrator_runner.py)

**Role:**
- Compiles all agent outputs into structured evidence map
- Builds agent execution chain
- Calculates evidence weights
- Stores as versioned artifact (optional)

**Generation Flow:**
```python
async def compile_evidence_map(self, session_id: str) -> Dict:
    """
    Compile all agent outputs into comprehensive evidence map
    """
    # 1. Retrieve all agent outputs from session
    agent_outputs = self.storage.get_agent_outputs(session_id)

    # 2. Extract decision from recommendation_agent
    recommendation = self._get_agent_output(agent_outputs, "recommendation_agent")

    # 3. Build supporting evidence with weights
    supporting_evidence = []
    for agent_id in ["intake_agent", "coverage_agent", "fraud_agent", "severity_agent"]:
        output = self._get_agent_output(agent_outputs, agent_id)
        if output:
            supporting_evidence.append({
                "agent_id": agent_id,
                "agent_name": self.registry.get_agent(agent_id)["name"],
                "findings": output.findings,
                "confidence": output.confidence,
                "weight": self._calculate_evidence_weight(agent_id),
                "key_data_points": output.key_data_points
            })

    # 4. Extract human interventions
    human_interventions = self.storage.get_checkpoint_events(session_id)

    # 5. Build agent execution chain
    agent_chain = self._build_execution_sequence(session_id)

    # 6. Compile assumptions and limitations
    assumptions = self._extract_assumptions(agent_outputs)
    limitations = self._extract_limitations(agent_outputs)

    # 7. Assemble complete evidence map
    evidence_map = {
        "decision": {
            "outcome": recommendation.outcome,
            "confidence_score": recommendation.confidence_score,
            "rationale": recommendation.rationale,
            "financial_impact": {
                "claim_amount": recommendation.claim_amount,
                "approved_amount": recommendation.approved_amount,
                "adjustment_reason": recommendation.adjustment_reason
            }
        },
        "supporting_evidence": supporting_evidence,
        "human_interventions": human_interventions,
        "agent_execution_chain": agent_chain,
        "assumptions": assumptions,
        "limitations": limitations,
        "metadata": {
            "session_id": session_id,
            "generated_at": datetime.utcnow().isoformat(),
            "total_agents_invoked": len(agent_outputs),
            "total_execution_time_seconds": self._calculate_total_time(session_id)
        }
    }

    # 8. Store as versioned artifact if enabled
    if self.config.artifact_versioning_enabled:
        await self.artifact_store.create_version(
            artifact_id=f"{session_id}_evidence",
            content=evidence_map,
            metadata={
                "type": "evidence_map",
                "session_id": session_id,
                "created_by": "orchestrator_agent"
            }
        )

    return evidence_map
```

#### Supporting: Explainability Agent (Optional)
**Config:** `registries/agent_registry.json` → `explainability_agent`

**Role:**
- Enhances evidence map with natural language explanations
- Formats for specific audiences (technical, business, regulatory)
- Adds compliance notes and references

**Example Enhancement:**
```python
async def enhance_evidence_map(
    base_evidence: Dict,
    target_audience: str = "business"
) -> Dict:
    """
    Enhance evidence map with natural language explanations
    """
    # Invoke explainability_agent to generate explanations
    enhanced_evidence = {
        **base_evidence,
        "narrative_explanation": {
            "executive_summary": "...",
            "detailed_analysis": "...",
            "regulatory_compliance": "...",
            "customer_communication": "..."
        }
    }
    return enhanced_evidence
```

### Evidence Map Schema

**Complete Structure:**

```json
{
  "decision": {
    "outcome": "APPROVE_WITH_ADJUSTMENTS",
    "confidence_score": 87.5,
    "rationale": "Claim is valid but coverage limits require adjustment. No fraud indicators detected. Property damage confirmed within policy terms.",
    "financial_impact": {
      "claim_amount": 15000.00,
      "approved_amount": 12500.00,
      "adjustment_reason": "Coverage limit exceeded by $2,500",
      "currency": "USD"
    }
  },

  "supporting_evidence": [
    {
      "agent_id": "intake_agent",
      "agent_name": "Claims Intake Specialist",
      "findings": {
        "claimant_verified": true,
        "policy_active": true,
        "incident_date_valid": true,
        "documentation_complete": true
      },
      "confidence": 95.0,
      "weight": 0.20,
      "key_data_points": [
        "Policy #POL-2024-5678 active",
        "Incident occurred within coverage period",
        "All required documentation submitted"
      ]
    },
    {
      "agent_id": "coverage_agent",
      "agent_name": "Coverage Verification Specialist",
      "findings": {
        "coverage_type": "Property Damage",
        "coverage_limit": 12500.00,
        "deductible": 500.00,
        "exclusions_apply": false,
        "sublimits_apply": true
      },
      "confidence": 92.0,
      "weight": 0.25,
      "key_data_points": [
        "Property damage coverage confirmed",
        "Coverage limit: $12,500",
        "No exclusions apply to this incident"
      ]
    },
    {
      "agent_id": "fraud_agent",
      "agent_name": "Fraud Detection Specialist",
      "findings": {
        "fraud_score": 15.0,
        "risk_level": "LOW",
        "indicators_detected": [],
        "recommendation": "No fraud concerns"
      },
      "confidence": 88.0,
      "weight": 0.35,
      "key_data_points": [
        "No suspicious patterns detected",
        "Claimant history clean",
        "Incident details consistent"
      ]
    },
    {
      "agent_id": "severity_agent",
      "agent_name": "Claim Severity Analyst",
      "findings": {
        "severity_score": 65.0,
        "complexity": "MODERATE",
        "processing_priority": "STANDARD",
        "estimated_processing_time_days": 5
      },
      "confidence": 90.0,
      "weight": 0.20,
      "key_data_points": [
        "Moderate complexity claim",
        "Standard processing timeline",
        "No special handling required"
      ]
    }
  ],

  "human_interventions": [
    {
      "intervention_id": "int_001",
      "checkpoint_id": "coverage_verification_review",
      "timestamp": "2024-01-05T10:35:00Z",
      "approver": "jane.smith@insurance.com",
      "approver_role": "Senior Claims Adjuster",
      "decision": "APPROVED",
      "notes": "Coverage limit adjustment verified. Approved for processing.",
      "fields_modified": []
    }
  ],

  "agent_execution_chain": [
    {
      "sequence": 1,
      "agent_id": "intake_agent",
      "started_at": "2024-01-05T10:30:00Z",
      "completed_at": "2024-01-05T10:30:15Z",
      "duration_seconds": 15,
      "status": "completed",
      "tools_used": ["policy_snapshot", "claimant_history"]
    },
    {
      "sequence": 2,
      "agent_id": "coverage_agent",
      "started_at": "2024-01-05T10:30:15Z",
      "completed_at": "2024-01-05T10:30:45Z",
      "duration_seconds": 30,
      "status": "completed",
      "tools_used": ["policy_snapshot", "coverage_calculator"],
      "context_received_from": ["intake_agent"]
    },
    {
      "sequence": 3,
      "agent_id": "fraud_agent",
      "started_at": "2024-01-05T10:30:45Z",
      "completed_at": "2024-01-05T10:31:30Z",
      "duration_seconds": 45,
      "status": "completed",
      "tools_used": ["fraud_rules", "similarity_search"],
      "context_received_from": ["intake_agent", "coverage_agent"]
    },
    {
      "sequence": 4,
      "agent_id": "severity_agent",
      "started_at": "2024-01-05T10:31:30Z",
      "completed_at": "2024-01-05T10:32:00Z",
      "duration_seconds": 30,
      "status": "completed",
      "tools_used": ["severity_calculator"],
      "context_received_from": ["intake_agent", "fraud_agent"]
    },
    {
      "sequence": 5,
      "agent_id": "recommendation_agent",
      "started_at": "2024-01-05T10:32:00Z",
      "completed_at": "2024-01-05T10:32:30Z",
      "duration_seconds": 30,
      "status": "completed",
      "tools_used": [],
      "context_received_from": ["coverage_agent", "fraud_agent", "severity_agent"]
    }
  ],

  "assumptions": [
    {
      "assumption": "Policy details from policy_snapshot are current and accurate",
      "impact_if_invalid": "Coverage determination may be incorrect",
      "confidence": "HIGH"
    },
    {
      "assumption": "Fraud detection rules are up-to-date with latest patterns",
      "impact_if_invalid": "Fraud risk may be underestimated",
      "confidence": "MEDIUM"
    },
    {
      "assumption": "All submitted documentation is authentic",
      "impact_if_invalid": "Claim validity may be compromised",
      "confidence": "HIGH"
    }
  ],

  "limitations": [
    {
      "limitation": "Cannot physically inspect property damage",
      "mitigation": "Relies on submitted photos and third-party reports",
      "impact": "MEDIUM"
    },
    {
      "limitation": "Historical fraud patterns may not cover emerging schemes",
      "mitigation": "Fraud rules updated quarterly",
      "impact": "LOW"
    },
    {
      "limitation": "Coverage interpretation based on policy text parsing",
      "mitigation": "Human review checkpoint for complex cases",
      "impact": "LOW"
    }
  ],

  "metadata": {
    "session_id": "sess_abc123",
    "generated_at": "2024-01-05T10:32:30.000Z",
    "total_agents_invoked": 5,
    "total_execution_time_seconds": 150,
    "workflow_id": "claims_triage",
    "system_version": "1.0.0"
  }
}
```

### Evidence Weight Calculation

**Algorithm:**
```python
def _calculate_evidence_weight(agent_id: str) -> float:
    """
    Calculate evidence weight based on agent type and workflow position
    """
    # Default weights by agent type
    weight_map = {
        "intake_agent": 0.20,      # Foundation data
        "coverage_agent": 0.25,    # Critical for decision
        "fraud_agent": 0.35,       # High impact on outcome
        "severity_agent": 0.20,    # Operational impact
        "recommendation_agent": 1.0  # Final synthesizer
    }

    return weight_map.get(agent_id, 0.15)
```

### Use Cases

1. **Regulatory Compliance:**
   - Prove decision transparency
   - Document audit trail
   - Show human oversight

2. **Customer Communication:**
   - Explain claim decisions
   - Provide detailed justification
   - Build trust

3. **Quality Assurance:**
   - Review decision quality
   - Identify process improvements
   - Train new adjusters

4. **Dispute Resolution:**
   - Reference evidence during appeals
   - Show decision rationale
   - Support legal proceedings

---

## Data Flow Architecture

### End-to-End Flow

```
┌──────────────────────────────────────────────────────────────┐
│  1. WORKFLOW EXECUTION                                       │
│     User submits claim → Orchestrator starts execution       │
└────────────────────┬─────────────────────────────────────────┘
                     │
                     ▼
┌──────────────────────────────────────────────────────────────┐
│  2. CONTEXT COMPILATION (per agent invocation)               │
│     ┌────────────────────────────────────────────────┐      │
│     │  ContextProcessorPipeline.execute()            │      │
│     │  ├─ Execute 7 processors                       │      │
│     │  ├─ Track execution metrics                    │      │
│     │  └─ Return compiled context                    │      │
│     └────────────────┬───────────────────────────────┘      │
│                      │                                        │
│     ┌────────────────▼───────────────────────────────┐      │
│     │  ContextLineageTracker.log_compilation()       │      │
│     │  └─ Write to context_lineage.jsonl             │      │
│     └────────────────────────────────────────────────┘      │
└──────────────────────────────────────────────────────────────┘
                     │
                     ▼
┌──────────────────────────────────────────────────────────────┐
│  3. TOKEN ANALYTICS COLLECTION                               │
│     ┌────────────────────────────────────────────────┐      │
│     │  TokenBudgetEnforcer.process()                 │      │
│     │  └─ Log truncation events                      │      │
│     └────────────────────────────────────────────────┘      │
│     ┌────────────────────────────────────────────────┐      │
│     │  CompactionManager.compact()                   │      │
│     │  └─ Log compaction events                      │      │
│     └────────────────────────────────────────────────┘      │
│     ┌────────────────────────────────────────────────┐      │
│     │  LLMClient.call_llm()                          │      │
│     │  └─ Log actual token usage                     │      │
│     └────────────────────────────────────────────────┘      │
│                                                              │
│     All write to session.jsonl                               │
└──────────────────────────────────────────────────────────────┘
                     │
                     ▼
┌──────────────────────────────────────────────────────────────┐
│  4. EVIDENCE MAP GENERATION                                  │
│     ┌────────────────────────────────────────────────┐      │
│     │  OrchestratorRunner.compile_evidence_map()     │      │
│     │  ├─ Collect all agent outputs                  │      │
│     │  ├─ Build execution chain                      │      │
│     │  ├─ Calculate weights                          │      │
│     │  └─ Compile 6-section structure                │      │
│     └────────────────┬───────────────────────────────┘      │
│                      │                                        │
│     ┌────────────────▼───────────────────────────────┐      │
│     │  ArtifactVersionStore.create_version()         │      │
│     │  └─ Store as artifacts/sess_id_evidence.json   │      │
│     └────────────────────────────────────────────────┘      │
└──────────────────────────────────────────────────────────────┘
                     │
                     ▼
┌──────────────────────────────────────────────────────────────┐
│  5. API LAYER                                                │
│     ┌────────────────────────────────────────────────┐      │
│     │  GET /sessions/{id}/context-stats              │      │
│     │  ├─ Read session.jsonl                         │      │
│     │  ├─ Read context_lineage.jsonl                 │      │
│     │  ├─ Aggregate analytics                        │      │
│     │  └─ Return JSON response                       │      │
│     └────────────────────────────────────────────────┘      │
│     ┌────────────────────────────────────────────────┐      │
│     │  GET /sessions/{id}/evidence                   │      │
│     │  ├─ Read artifacts/sess_id_evidence.json       │      │
│     │  └─ Return evidence map                        │      │
│     └────────────────────────────────────────────────┘      │
└────────────────────┬─────────────────────────────────────────┘
                     │
                     ▼
┌──────────────────────────────────────────────────────────────┐
│  6. FRONTEND DISPLAY                                         │
│     ┌────────────────────────────────────────────────┐      │
│     │  Token Analytics Tab                           │      │
│     │  ├─ Fetch /context-stats                       │      │
│     │  ├─ Render ContextTimeline                     │      │
│     │  ├─ Render TokenBudgetChart                    │      │
│     │  └─ Render ContextLineageTree                  │      │
│     └────────────────────────────────────────────────┘      │
│     ┌────────────────────────────────────────────────┐      │
│     │  Explainability Tab                            │      │
│     │  ├─ Fetch /evidence                            │      │
│     │  ├─ Render 6-section evidence display          │      │
│     │  └─ Show InfoTooltips                          │      │
│     └────────────────────────────────────────────────┘      │
└──────────────────────────────────────────────────────────────┘
```

---

## API Reference

### Context Stats Endpoint

**Request:**
```http
GET /sessions/{session_id}/context-stats
Authorization: Bearer {token}
```

**Response:**
```json
{
  "session_id": "sess_abc123",
  "analytics": {
    "compilation_stats": { ... },
    "token_usage": { ... },
    "truncation_events": { ... },
    "compaction_events": { ... },
    "processor_performance": { ... },
    "cost_estimates": { ... }
  },
  "timeline": [ ... ]
}
```

### Evidence Endpoint

**Request:**
```http
GET /sessions/{session_id}/evidence
Authorization: Bearer {token}
```

**Response:**
```json
{
  "decision": { ... },
  "supporting_evidence": [ ... ],
  "human_interventions": [ ... ],
  "agent_execution_chain": [ ... ],
  "assumptions": [ ... ],
  "limitations": [ ... ],
  "metadata": { ... }
}
```

### Manual Compaction Trigger

**Request:**
```http
POST /sessions/{session_id}/compact
Authorization: Bearer {token}
Content-Type: application/json

{
  "force": true
}
```

**Response:**
```json
{
  "session_id": "sess_abc123",
  "compaction_status": "completed",
  "tokens_before": 15000,
  "tokens_after": 5000,
  "compression_ratio": 0.33
}
```

---

## Frontend Components

### Token Analytics Tab

**File:** [`frontend/app/replay/[sessionId]/page.tsx`](frontend/app/replay/[sessionId]/page.tsx)

**Components:**

#### 1. ContextTimeline
**File:** [`frontend/components/visualization/ContextTimeline.tsx`](frontend/components/visualization/ContextTimeline.tsx)

**Visualization:**
- Line chart showing token usage over time
- X-axis: Timestamp
- Y-axis: Token count
- Shows budget limit as horizontal line
- Highlights truncation events

**Props:**
```typescript
interface ContextTimelineProps {
  timeline: Array<{
    timestamp: string;
    agent_id: string;
    tokens_used: number;
    truncation_applied: boolean;
    compaction_applied: boolean;
  }>;
  budgetLimit: number;
}
```

#### 2. TokenBudgetChart
**File:** [`frontend/components/visualization/TokenBudgetChart.tsx`](frontend/components/visualization/TokenBudgetChart.tsx)

**Visualization:**
- Stacked bar chart showing token distribution
- Categories: Input Context, Prior Outputs, Tool Observations
- Shows budget allocation percentages
- Color-coded by category

**Props:**
```typescript
interface TokenBudgetChartProps {
  budgetAllocation: {
    input_context: number;
    prior_outputs: number;
    tool_observations: number;
  };
  budgetLimits: {
    input_budget: number;
    outputs_budget: number;
    observations_budget: number;
  };
}
```

#### 3. ContextLineageTree
**File:** [`frontend/components/visualization/ContextLineageTree.tsx`](frontend/components/visualization/ContextLineageTree.tsx)

**Visualization:**
- Tree diagram showing processor execution
- Each node: processor name, execution time, success status
- Shows data flow between processors
- Expandable to show modifications made

**Props:**
```typescript
interface ContextLineageTreeProps {
  processors: Array<{
    processor_id: string;
    order: number;
    execution_time_ms: number;
    success: boolean;
    modifications_made: Record<string, any>;
  }>;
}
```

### Explainability Tab

**File:** [`frontend/app/replay/[sessionId]/page.tsx`](frontend/app/replay/[sessionId]/page.tsx)

**Structure:**
- 6 collapsible sections
- InfoTooltip for every field
- Visual agent execution chain
- Demo fallback when data incomplete

**Key Features:**
- **Decision Section**: Outcome, confidence, financial impact
- **Supporting Evidence**: Agent findings with weights (color-coded)
- **Human Interventions**: Audit trail with timestamps
- **Agent Chain**: Visual sequence diagram
- **Assumptions**: Impact analysis
- **Limitations**: Mitigation strategies

---

## Usage Examples

### Example 1: Debug Slow Context Compilation

**Problem:** Context compilation taking too long

**Solution:**
```bash
# 1. View context lineage
cat storage/sessions/sess_abc123_context_lineage.jsonl | jq

# 2. Identify slow processor
cat storage/sessions/sess_abc123_context_lineage.jsonl | \
  jq '.data.processors_executed[] | {processor: .processor_id, time: .execution_time_ms}' | \
  sort -k2 -rn

# Output shows memory_retriever taking 45ms
# → Optimize memory similarity search algorithm
```

### Example 2: Optimize Token Costs

**Problem:** High token usage driving up costs

**Solution:**
1. View token analytics in frontend
2. Identify high token agents (fraud_agent using 10K tokens)
3. Check truncation events
4. Adjust budget allocation in `context_strategies.json`:
```json
{
  "token_budgets": {
    "default_budget": 8000,
    "agent_overrides": {
      "fraud_agent": 6000
    }
  }
}
```

### Example 3: Generate Customer Explanation

**Problem:** Customer disputes claim denial

**Solution:**
```bash
# 1. Fetch evidence map
curl http://localhost:8016/sessions/sess_abc123/evidence | jq

# 2. Extract rationale
curl http://localhost:8016/sessions/sess_abc123/evidence | \
  jq '.decision.rationale'

# 3. Show supporting evidence
curl http://localhost:8016/sessions/sess_abc123/evidence | \
  jq '.supporting_evidence[] | {agent: .agent_name, findings: .key_data_points}'

# 4. Use in customer communication
```

---

## Configuration

### Enable Context Engineering Features

**File:** `registries/system_config.json`

```json
{
  "context_engineering": {
    "enabled": true,
    "features": {
      "context_lineage_tracking": true,
      "token_analytics": true,
      "explainability": true
    }
  }
}
```

### Configure Token Budgets

**File:** `registries/context_strategies.json`

```json
{
  "token_budgets": {
    "default_budget": 8000,
    "budget_allocation": {
      "input_context": 0.30,
      "prior_outputs": 0.50,
      "tool_observations": 0.20
    },
    "agent_overrides": {
      "fraud_agent": 10000,
      "recommendation_agent": 12000
    }
  }
}
```

### Configure Context Processors

**File:** `registries/context_processor_pipeline.json`

```json
{
  "processors": [
    {
      "processor_id": "content_selector",
      "enabled": true,
      "order": 1,
      "config": {
        "filter_debug_events": true,
        "filter_system_events": false
      }
    },
    {
      "processor_id": "token_budget_enforcer",
      "enabled": true,
      "order": 6,
      "config": {
        "truncation_strategy": "prioritize_recent",
        "min_context_tokens": 1000
      }
    }
  ]
}
```

### Enable Compaction

**File:** `registries/context_strategies.json`

```json
{
  "compaction": {
    "enabled": true,
    "token_threshold": 8000,
    "event_count_threshold": 100,
    "compaction_strategy": "summarize_with_llm",
    "preserve_recent_events": 20
  }
}
```

---

## Related Documentation

- [CONFIGURATION_EXECUTION_LAYERS.md](CONFIGURATION_EXECUTION_LAYERS.md) - Which layers generate observability data
- [CLAUDE.md](CLAUDE.md) - Developer guide with configuration examples
- [docs/USER_GUIDE_CONTEXT_ENGINEERING.md](docs/USER_GUIDE_CONTEXT_ENGINEERING.md) - How to use context features
- [docs/DEVELOPER_GUIDE_CONTEXT_PROCESSORS.md](docs/DEVELOPER_GUIDE_CONTEXT_PROCESSORS.md) - Create custom processors
- [EXPLAINABILITY_TAB_DOCUMENTATION.md](EXPLAINABILITY_TAB_DOCUMENTATION.md) - Complete explainability guide

---

## Summary

| Feature | Purpose | Generated By | Storage | API | Frontend |
|---------|---------|--------------|---------|-----|----------|
| **Context Engineering Log** | Debug pipeline, audit decisions | ContextLineageTracker, ContextProcessorPipeline | `{session_id}_context_lineage.jsonl` | `/sessions/{id}/context-stats` | Token Analytics → Context Lineage Tree |
| **Token Analytics** | Optimize costs, monitor usage | TokenBudgetEnforcer, CompactionManager, LLMClient | Session JSONL + lineage JSONL | `/sessions/{id}/context-stats` | Token Analytics → 3 charts |
| **Explainability** | Business justification, compliance | OrchestratorRunner, Explainability Agent | `artifacts/{session_id}_evidence.json` | `/sessions/{id}/evidence` | Explainability → 6 sections |

These three observability features provide complete transparency into AgentMesh's context engineering system, enabling debugging, optimization, and business justification at all levels.
