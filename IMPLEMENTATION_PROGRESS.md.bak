# AgentMesh Implementation Progress

**Production-Scale Multi-Agentic Insurance Framework**

This document provides a comprehensive overview of all implementation work completed through Phases 1-3, demonstrating scalability patterns for productionizing multi-agent solutions.

---

## Table of Contents

1. [Phase 1: Foundation & Registries](#phase-1-foundation--registries)
2. [Phase 2: Core Orchestration Services](#phase-2-core-orchestration-services)
3. [Phase 3: LLM Integration](#phase-3-llm-integration)
4. [Architecture Overview](#architecture-overview)
5. [Key Scalability Patterns](#key-scalability-patterns)

---

## Phase 1: Foundation & Registries

**Goal**: Establish configuration-driven foundation with complete registry system for dynamic discovery.

**Status**: ✅ Complete

### 1.1 Project Structure

Created complete directory structure for dockerized multi-service architecture:

```
AgentMesh/
├── .env                                    # Single source of configuration truth
├── docker-compose.yml                      # Multi-service orchestration
├── registries/                             # Configuration-driven registries
│   ├── agent_registry.json                 # Agent definitions & capabilities
│   ├── tool_registry.json                  # Tool catalog
│   ├── model_profiles.json                 # LLM model configurations
│   ├── governance_policies.json            # Access control policies
│   └── workflows/
│       └── claims_triage.json              # Workflow definitions
├── storage/                                # Persistent storage
│   ├── sessions/                           # JSONL event streams
│   └── artifacts/                          # Evidence maps
├── backend/orchestrator/                   # Central control plane
│   ├── Dockerfile
│   ├── requirements.txt
│   └── app/
│       ├── config.py                       # Centralized configuration
│       ├── services/                       # Core services
│       │   ├── storage.py                  # JSONL/JSON operations
│       │   ├── registry_manager.py         # Dynamic discovery
│       │   ├── governance_enforcer.py      # Policy enforcement
│       │   ├── context_compiler.py         # Context management
│       │   ├── agent_react_loop.py         # Worker agent loops
│       │   ├── orchestrator_runner.py      # Meta-agent loop
│       │   ├── llm_client.py               # Multi-provider LLM
│       │   └── response_parser.py          # JSON parsing
│       └── prompts/
│           └── react_prompts.py            # ReAct prompt engineering
├── tools/tools_gateway/                    # Tools service
└── frontend/                               # Next.js UI
```

**Key Decisions**:
- **Flat-file persistence**: JSONL for events, JSON for artifacts (simplifies Phase 1, production would use DB)
- **Registry-driven**: All behavior configured via JSON files, not code
- **Single .env file**: Centralized configuration with environment variable overrides

---

### 1.2 Docker Configuration

**File**: `docker-compose.yml`

**Services Defined**:

1. **Orchestrator** (FastAPI):
   - Port: `8016` (external) → `8000` (internal)
   - Volumes: App code (hot reload), registries (read-only), storage (read-write)
   - Environment: OpenAI + Anthropic API keys, storage path
   - Health check: `curl -f http://localhost:8000/health`

2. **Tools Gateway** (FastAPI):
   - Port: `8010`
   - Volumes: App code, registries (read-only)
   - Internal service name: `tools_gateway` (for Docker network)

3. **Frontend** (Next.js):
   - Port: `3016` (external) → `3000` (internal)
   - Environment: `NEXT_PUBLIC_ORCHESTRATOR_URL=http://localhost:8016`
   - Volumes: App code with node_modules persistence

**Network**: `agentmesh` bridge network for inter-service communication

**Scalability Pattern**: Services communicate via container names on internal network, enabling horizontal scaling and service discovery.

---

### 1.3 Environment Configuration

**File**: `.env`

**Configuration Categories**:

```bash
# LLM Provider Configuration
OPENAI_API_KEY=your-openai-api-key-here
ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Service URLs (Internal Docker Network)
TOOLS_BASE_URL=http://tools_gateway:8010

# Orchestrator + Frontend Ports
ORCHESTRATOR_PORT=8016
FRONTEND_PORT=3016
NEXT_PUBLIC_ORCHESTRATOR_URL=http://localhost:8016

# Orchestrator Agent Limits
ORCHESTRATOR_MAX_ITERATIONS=10
ORCHESTRATOR_ITERATION_TIMEOUT_SECONDS=30

# Workflow Execution Limits
WORKFLOW_MAX_DURATION_SECONDS=300
WORKFLOW_MAX_AGENT_INVOCATIONS=20

# Agent Execution Limits (defaults)
AGENT_DEFAULT_MAX_ITERATIONS=5
AGENT_DEFAULT_ITERATION_TIMEOUT_SECONDS=30
AGENT_MAX_DUPLICATE_INVOCATIONS=2

# LLM Constraints
LLM_TIMEOUT_SECONDS=30
LLM_MAX_RETRIES=3
LLM_MAX_TOKENS_PER_REQUEST=2000
LLM_MAX_TOKENS_PER_SESSION=50000

# Governance Limits
MAX_TOOL_INVOCATIONS_PER_SESSION=50
MAX_LLM_CALLS_PER_SESSION=30

# Safety Thresholds
CONSECUTIVE_NO_PROGRESS_LIMIT=2
MALFORMED_RESPONSE_LIMIT=3
```

**Scalability Pattern**: All limits configurable via environment variables, enabling production tuning without code changes.

---

### 1.4 Agent Registry

**File**: `registries/agent_registry.json`

**Purpose**: Define all agents with capabilities, tool access, and execution constraints.

**Key Entries**:

#### 1.4.1 Orchestrator Agent

```json
{
  "agent_id": "orchestrator_agent",
  "name": "Orchestrator Agent",
  "description": "Meta-agent that coordinates claim processing by dynamically selecting and invoking appropriate agents based on workflow state, agent capabilities, and analysis requirements. Discovers agents from registry and decides execution order.",
  "capabilities": [
    "workflow_coordination",
    "agent_selection",
    "context_management",
    "decision_routing",
    "dynamic_orchestration"
  ],
  "allowed_agents": [
    "intake_agent",
    "coverage_agent",
    "fraud_agent",
    "severity_agent",
    "recommendation_agent",
    "explainability_agent"
  ],
  "model_profile_id": "default_gpt35",
  "max_iterations": 10,
  "iteration_timeout_seconds": 30,
  "output_schema": {
    "type": "object",
    "required": ["status", "evidence_map", "agents_executed"],
    "properties": {
      "status": {
        "type": "string",
        "enum": ["completed", "completed_with_warning", "failed"]
      },
      "completion_reason": {
        "type": "string",
        "enum": [
          "all_objectives_achieved",
          "max_iterations_reached",
          "timeout",
          "token_budget_exceeded",
          "error"
        ]
      },
      "evidence_map": {
        "type": "object",
        "required": ["decision", "supporting_evidence", "assumptions", "limitations"],
        "properties": {
          "decision": {"type": "object"},
          "supporting_evidence": {"type": "array"},
          "assumptions": {"type": "array"},
          "limitations": {"type": "array"},
          "agent_chain": {"type": "array"}
        }
      },
      "agents_executed": {
        "type": "array",
        "items": {"type": "string"}
      }
    }
  }
}
```

**Design Decisions**:
- **Orchestrator is a ReAct agent**: Discovers and invokes other agents dynamically (not hardcoded)
- **allowed_agents**: Governance-enforced list of invokable agents
- **Output schema**: Structured evidence map for explainability
- **Higher iteration limit**: 10 vs 5 for workers (coordinates multiple agents)

#### 1.4.2 Worker Agents (Example: Fraud Agent)

```json
{
  "agent_id": "fraud_agent",
  "name": "Fraud Signal Agent",
  "description": "Evaluates fraud indicators using rule-based and heuristic analysis",
  "capabilities": [
    "fraud_detection",
    "risk_scoring",
    "pattern_analysis",
    "anomaly_detection"
  ],
  "allowed_tools": [
    "fraud_rules",
    "similarity"
  ],
  "model_profile_id": "default_gpt35",
  "max_iterations": 5,
  "iteration_timeout_seconds": 45,
  "output_schema": {
    "type": "object",
    "required": ["fraud_score", "risk_band", "triggered_indicators", "rationale"],
    "properties": {
      "fraud_score": {
        "type": "number",
        "minimum": 0,
        "maximum": 1
      },
      "risk_band": {
        "type": "string",
        "enum": ["low", "medium", "high", "critical"]
      },
      "triggered_indicators": {
        "type": "array",
        "items": {
          "type": "object",
          "properties": {
            "indicator_id": {"type": "string"},
            "indicator_name": {"type": "string"},
            "severity": {"type": "string"}
          }
        }
      },
      "rationale": {"type": "string"}
    }
  },
  "context_requirements": {
    "requires_prior_outputs": ["intake", "coverage"],
    "max_context_tokens": 6000
  }
}
```

**Design Decisions**:
- **allowed_tools**: Agent can only discover and use fraud-related tools
- **context_requirements**: Explicitly declares dependencies on prior agent outputs
- **Specialized model**: Each agent can use different LLM via `model_profile_id`

**All 6 Worker Agents**:
1. **intake_agent**: Validates and normalizes claim data
2. **coverage_agent**: Determines policy coverage and exclusions
3. **fraud_agent**: Detects fraud signals
4. **severity_agent**: Assesses claim complexity
5. **recommendation_agent**: Determines next best action
6. **explainability_agent**: Compiles evidence map

**Scalability Pattern**: Agent-level model selection enables cost optimization (cheap models for simple tasks, expensive for complex).

---

### 1.5 Tool Registry

**File**: `registries/tool_registry.json`

**Purpose**: Catalog all available tools with LLM-friendly descriptions for dynamic discovery.

**Example Entry**:

```json
{
  "tool_id": "fraud_rules",
  "name": "Fraud Rules Engine",
  "description": "Evaluates claim against configurable fraud detection rules. Returns triggered rule IDs, severity levels, and confidence scores. Use this when you need to check for known fraud patterns (duplicate claims, provider anomalies, timeline inconsistencies, amount outliers).",
  "endpoint": "http://tools_gateway:8010/invoke/fraud_rules",
  "input_schema": {
    "type": "object",
    "required": ["claim_id", "claim_data"],
    "properties": {
      "claim_id": {
        "type": "string",
        "description": "Unique claim identifier"
      },
      "claim_data": {
        "type": "object",
        "description": "Full claim data object including policy, claimant, incident details"
      },
      "rule_categories": {
        "type": "array",
        "items": {"type": "string"},
        "description": "Optional: Filter to specific rule categories (e.g., 'duplicate', 'provider', 'timeline', 'amount')"
      }
    }
  },
  "output_schema": {
    "type": "object",
    "properties": {
      "triggered_rules": {"type": "array"},
      "overall_fraud_score": {"type": "number"},
      "risk_band": {"type": "string"}
    }
  },
  "lineage_tags": ["fraud_detection", "rules_engine", "risk_assessment"]
}
```

**Design Decisions**:
- **LLM-friendly descriptions**: Tools describe WHEN to use them (for agent reasoning)
- **Detailed input schemas**: Agents know what parameters to provide
- **Lineage tags**: Enable tool filtering/discovery by category
- **HTTP endpoints**: Tools are services, not hardcoded functions (scalable)

**All Tools Defined**:
1. `schema_validator`: Validates claim data structure
2. `policy_snapshot`: Retrieves policy coverage details
3. `coverage_rules`: Determines coverage eligibility
4. `fraud_rules`: Fraud detection rules engine
5. `similarity`: Finds similar historical claims
6. `decision_rules`: Recommends actions based on analysis

---

### 1.6 Model Profiles

**File**: `registries/model_profiles.json`

**Purpose**: Define LLM model configurations for multi-provider support.

**OpenAI Profiles**:

```json
{
  "profile_id": "default_gpt35",
  "name": "Default GPT-3.5 Turbo",
  "description": "Cost-optimized model for general agent reasoning and tool selection",
  "provider": "openai",
  "model_name": "gpt-3.5-turbo",
  "intended_usage": "general_reasoning",
  "parameters": {
    "temperature": 0.3,
    "max_tokens": 2000,
    "top_p": 1.0,
    "frequency_penalty": 0.0,
    "presence_penalty": 0.0
  },
  "json_mode": true,
  "constraints": {
    "max_context_tokens": 16385,
    "max_output_tokens": 4096
  },
  "retry_policy": {
    "max_retries": 3,
    "backoff_multiplier": 2,
    "initial_delay_ms": 1000
  },
  "timeout_seconds": 30
}
```

**Claude Profiles**:

```json
{
  "profile_id": "claude_sonnet_35",
  "name": "Claude 3.5 Sonnet",
  "description": "Anthropic's most intelligent model - best for complex analysis and reasoning",
  "provider": "anthropic",
  "model_name": "claude-3-5-sonnet-20241022",
  "intended_usage": "complex_reasoning_highest_quality",
  "parameters": {
    "temperature": 0.3,
    "max_tokens": 4000,
    "top_p": 1.0
  },
  "json_mode": false,
  "constraints": {
    "max_context_tokens": 200000,
    "max_output_tokens": 8192
  },
  "retry_policy": {
    "max_retries": 3,
    "backoff_multiplier": 2,
    "initial_delay_ms": 1000
  },
  "timeout_seconds": 60
}
```

**All Profiles**:
- **OpenAI**: `default_gpt35`, `gpt4`, `gpt4_turbo`
- **Claude**: `claude_sonnet_35`, `claude_opus_3`, `claude_haiku_3`

**Scalability Pattern**: Agent-level model selection via `model_profile_id` reference, not hardcoded model names.

---

### 1.7 Governance Policies

**File**: `registries/governance_policies.json`

**Purpose**: Define access control and execution constraints for production safety.

**Policy Categories**:

#### 1.7.1 Agent Invocation Access

```json
{
  "agent_invocation_access": {
    "description": "Defines which agents the orchestrator is permitted to invoke",
    "enforcement_level": "strict",
    "rules": [
      {
        "agent_id": "orchestrator_agent",
        "allowed_agents": [
          "intake_agent",
          "coverage_agent",
          "fraud_agent",
          "severity_agent",
          "recommendation_agent",
          "explainability_agent"
        ],
        "denied_agents": ["orchestrator_agent"]
      }
    ]
  }
}
```

**Purpose**: Prevent orchestrator from invoking itself (infinite loop), enforce which agents it can call.

#### 1.7.2 Agent Tool Access

```json
{
  "agent_tool_access": {
    "description": "Defines which tools each agent is permitted to request",
    "enforcement_level": "strict",
    "rules": [
      {
        "agent_id": "fraud_agent",
        "allowed_tools": ["fraud_rules", "similarity"],
        "denied_tools": ["policy_snapshot", "decision_rules", "coverage_rules"]
      }
    ]
  }
}
```

**Purpose**: Enforce principle of least privilege - agents only access tools needed for their role.

#### 1.7.3 Iteration Limits

```json
{
  "iteration_limits": {
    "description": "Maximum ReAct iterations per agent to prevent runaway loops",
    "enforcement_level": "strict",
    "global_max_iterations": 5,
    "agent_overrides": [
      {
        "agent_id": "orchestrator_agent",
        "max_iterations": 10,
        "_comment": "Orchestrator needs more iterations as it coordinates multiple agents"
      }
    ]
  }
}
```

**Purpose**: Bounded execution prevents infinite loops and cost explosion.

#### 1.7.4 Execution Constraints

```json
{
  "execution_constraints": {
    "description": "Global execution limits",
    "enforcement_level": "strict",
    "max_workflow_duration_seconds": 300,
    "max_tool_invocations_per_session": 50,
    "max_llm_calls_per_session": 30
  }
}
```

**Scalability Pattern**: Policy-driven governance enables runtime enforcement without code changes.

---

### 1.8 Workflow Definition

**File**: `registries/workflows/claims_triage.json`

**Purpose**: Define workflow in **advisory mode** - provides guidance but orchestrator can adapt.

**Key Sections**:

```json
{
  "workflow_id": "claims_triage",
  "name": "Claims Triage Workflow",
  "mode": "advisory",
  "goal": "Complete comprehensive claim triage with coverage, fraud, and recommendation analysis",

  "suggested_sequence": [
    "intake",
    "coverage",
    "fraud",
    "severity",
    "recommendation",
    "explainability"
  ],

  "required_agents": ["intake_agent", "explainability_agent"],
  "optional_agents": ["fraud_agent", "severity_agent"],

  "completion_criteria": {
    "required_outputs": ["evidence_map", "recommendation"],
    "required_agents_executed": ["intake_agent", "explainability_agent"],
    "min_agents_executed": 3,
    "evidence_map_required": true
  },

  "constraints": {
    "max_total_duration_seconds": 300,
    "max_orchestrator_iterations": 10,
    "max_agent_invocations": 20
  }
}
```

**Design Decisions**:
- **Advisory mode**: Orchestrator uses as guidance, not strict prescription
- **Flexible sequence**: Orchestrator can skip optional agents or reorder based on state
- **Clear completion criteria**: System knows when workflow is done
- **Configurable constraints**: Can override via environment variables

---

### 1.9 Storage System

**File**: `backend/orchestrator/app/services/storage.py`

**Purpose**: Thread-safe JSONL/JSON operations for event streaming and artifact storage.

**Key Components**:

#### 1.9.1 SessionWriter (JSONL Event Streams)

```python
class SessionWriter:
    """Thread-safe JSONL writer for session event streams."""

    def write_event(self, session_id: str, event: Dict[str, Any]) -> None:
        """Append event to session JSONL file (thread-safe)."""
        session_file = self.storage_path / "sessions" / f"{session_id}.jsonl"

        # File locking for thread safety
        with open(session_file, "a") as f:
            fcntl.flock(f.fileno(), fcntl.LOCK_EX)
            try:
                f.write(json.dumps(event) + "\n")
                f.flush()
                os.fsync(f.fileno())  # Ensure written to disk
            finally:
                fcntl.flock(f.fileno(), fcntl.LOCK_UN)
```

**Features**:
- **Append-only**: JSONL format for streaming events
- **Thread-safe**: File locking prevents race conditions
- **Immediate flush**: Events visible for real-time streaming

#### 1.9.2 SessionReader (Event Replay)

```python
class SessionReader:
    """Read session event streams for replay and analysis."""

    def read_session(self, session_id: str) -> List[Dict[str, Any]]:
        """Read complete session event timeline."""
        session_file = self.storage_path / "sessions" / f"{session_id}.jsonl"

        events = []
        with open(session_file, "r") as f:
            for line in f:
                events.append(json.loads(line))
        return events

    def filter_events(
        self,
        session_id: str,
        event_type: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """Filter events by type."""
        events = self.read_session(session_id)
        if event_type:
            events = [e for e in events if e.get("event_type") == event_type]
        return events
```

#### 1.9.3 ArtifactWriter (Evidence Maps, Results)

```python
class ArtifactWriter:
    """Write structured artifacts (evidence maps, final results)."""

    def write_artifact(
        self,
        artifact_id: str,
        artifact_type: str,
        data: Dict[str, Any]
    ) -> str:
        """Write artifact with metadata."""
        artifact = {
            "artifact_id": artifact_id,
            "artifact_type": artifact_type,
            "created_at": datetime.utcnow().isoformat() + "Z",
            "data": data
        }

        artifact_path = self.storage_path / "artifacts" / f"{artifact_id}.json"
        with open(artifact_path, "w") as f:
            json.dump(artifact, f, indent=2)

        return str(artifact_path)
```

**Scalability Pattern**: Flat-file storage for Phase 1, but abstracted interface enables easy swap to database in production.

---

### 1.10 Configuration System

**File**: `backend/orchestrator/app/config.py`

**Purpose**: Centralized configuration with Pydantic validation.

**Structure**:

```python
class OrchestratorLimits(BaseModel):
    """Orchestrator agent execution limits."""
    max_iterations: int = Field(default=10)
    iteration_timeout_seconds: int = Field(default=30)

class WorkflowLimits(BaseModel):
    """Workflow execution limits."""
    max_duration_seconds: int = Field(default=300)
    max_agent_invocations: int = Field(default=20)

class AgentLimits(BaseModel):
    """Default agent execution limits."""
    default_max_iterations: int = Field(default=5)
    default_iteration_timeout_seconds: int = Field(default=30)
    max_duplicate_invocations: int = Field(default=2)

class LLMLimits(BaseModel):
    """LLM API constraints."""
    timeout_seconds: int = Field(default=30)
    max_retries: int = Field(default=3)
    max_tokens_per_request: int = Field(default=2000)
    max_tokens_per_session: int = Field(default=50000)

class GovernanceLimits(BaseModel):
    """Governance and safety limits."""
    max_tool_invocations_per_session: int = Field(default=50)
    max_llm_calls_per_session: int = Field(default=30)

class SafetyThresholds(BaseModel):
    """Safety mechanism thresholds."""
    consecutive_no_progress_limit: int = Field(default=2)
    malformed_response_limit: int = Field(default=3)

class Config(BaseModel):
    """Complete system configuration."""
    openai_api_key: str
    anthropic_api_key: str
    tools_base_url: str
    storage_path: str
    orchestrator: OrchestratorLimits
    workflow: WorkflowLimits
    agent: AgentLimits
    llm: LLMLimits
    governance: GovernanceLimits
    safety: SafetyThresholds

def load_config() -> Config:
    """Load configuration from environment variables."""
    return Config(
        openai_api_key=os.getenv("OPENAI_API_KEY", ""),
        anthropic_api_key=os.getenv("ANTHROPIC_API_KEY", ""),
        # ... all other configs from environment
    )

# Singleton pattern
_config: Optional[Config] = None

def get_config() -> Config:
    """Get singleton Config instance."""
    global _config
    if _config is None:
        _config = load_config()
    return _config
```

**Scalability Pattern**: Type-safe configuration with validation, single source of truth, singleton pattern for performance.

---

## Phase 2: Core Orchestration Services

**Goal**: Build production-grade ReAct pattern implementation with governance, context management, and complete observability.

**Status**: ✅ Complete

### 2.1 Registry Manager

**File**: `backend/orchestrator/app/services/registry_manager.py`

**Purpose**: Dynamic discovery system with hot-reload capability.

**Architecture**:

```python
class RegistryManager:
    """
    Production-grade registry manager.

    Demonstrates scalability patterns:
    - Lazy loading with caching
    - Thread-safe operations
    - Hot-reload support
    - Validation on load
    - Lookup optimization (O(1) by ID)
    """

    def __init__(self, registries_path: str = "/registries"):
        self.registries_path = Path(registries_path)
        self._lock = threading.RLock()

        # Caches (indexed by ID for O(1) lookup)
        self._agents: Dict[str, AgentMetadata] = {}
        self._tools: Dict[str, ToolMetadata] = {}
        self._models: Dict[str, ModelProfile] = {}
        self._workflows: Dict[str, WorkflowDefinition] = {}
        self._governance: Optional[GovernancePolicies] = None

        # Metadata
        self._loaded_at: Optional[datetime] = None
        self._load_count = 0
```

**Key Methods**:

#### 2.1.1 Load All Registries

```python
def load_all(self) -> None:
    """
    Load all registries from disk.

    Thread-safe, can be called multiple times for hot-reload.
    Validates all entries on load.
    """
    with self._lock:
        self._load_agents()
        self._load_tools()
        self._load_models()
        self._load_workflows()
        self._load_governance()

        self._loaded_at = datetime.utcnow()
        self._load_count += 1
```

**Validation**: Each entry validated with Pydantic on load, invalid entries logged but don't crash system.

#### 2.1.2 Agent Discovery

```python
def get_agent(self, agent_id: str) -> Optional[AgentMetadata]:
    """Get agent metadata by ID (O(1) lookup)."""
    with self._lock:
        return self._agents.get(agent_id)

def list_agents(self, capability: Optional[str] = None) -> List[AgentMetadata]:
    """
    List all agents, optionally filtered by capability.

    Demonstrates: Dynamic discovery for agent selection.
    """
    with self._lock:
        agents = list(self._agents.values())

        if capability:
            agents = [a for a in agents if capability in a.capabilities]

        return agents

def get_agents_for_orchestrator(self) -> List[AgentMetadata]:
    """
    Get all agents that orchestrator can invoke.

    Demonstrates: Governance-aware discovery.
    """
    orchestrator = self.get_agent("orchestrator_agent")
    if not orchestrator or not orchestrator.allowed_agents:
        return []

    with self._lock:
        return [
            self._agents[agent_id]
            for agent_id in orchestrator.allowed_agents
            if agent_id in self._agents
        ]
```

#### 2.1.3 Tool Discovery

```python
def get_tools_for_agent(self, agent_id: str) -> List[ToolMetadata]:
    """
    Get all tools an agent is allowed to use.

    Demonstrates: Governance-aware tool discovery.
    """
    agent = self.get_agent(agent_id)
    if not agent:
        return []

    with self._lock:
        return [
            self._tools[tool_id]
            for tool_id in agent.allowed_tools
            if tool_id in self._tools
        ]
```

#### 2.1.4 Governance Queries

```python
def is_agent_invocation_allowed(
    self,
    invoker_agent_id: str,
    target_agent_id: str
) -> bool:
    """
    Check if invoker agent can invoke target agent.

    Demonstrates: Runtime governance enforcement.
    """
    if not self._governance:
        return True  # Permissive if no policies loaded

    policy = self._governance.policies.get("agent_invocation_access", {})
    rules = policy.get("rules", [])

    for rule in rules:
        if rule.get("agent_id") == invoker_agent_id:
            allowed = rule.get("allowed_agents", [])
            denied = rule.get("denied_agents", [])

            if target_agent_id in denied:
                return False
            if target_agent_id in allowed:
                return True

    return False  # Deny by default if not explicitly allowed
```

**Scalability Patterns**:
- **O(1) lookups**: Dict-based caching by ID
- **Thread-safe**: R-Lock for concurrent access
- **Hot-reload**: Can reload without restart
- **Governance-aware**: Discovery respects policies

**Singleton Pattern**:

```python
_registry_manager: Optional[RegistryManager] = None

def init_registry_manager(registries_path: str = "/registries"):
    """Initialize registry manager singleton."""
    global _registry_manager
    _registry_manager = RegistryManager(registries_path)
    _registry_manager.load_all()

def get_registry_manager() -> RegistryManager:
    """Get singleton RegistryManager instance."""
    if _registry_manager is None:
        raise RuntimeError("RegistryManager not initialized")
    return _registry_manager

def reload_registries():
    """Hot-reload all registries without restart."""
    manager = get_registry_manager()
    manager.load_all()
    return manager.get_stats()
```

---

### 2.2 Governance Enforcer

**File**: `backend/orchestrator/app/services/governance_enforcer.py`

**Purpose**: Policy-driven enforcement with audit trails.

**Architecture**:

```python
class GovernanceEnforcer:
    """
    Production-grade governance enforcer.

    Demonstrates scalability patterns:
    - Stateless enforcement (scales horizontally)
    - Policy-driven decisions
    - Audit trail for compliance
    - Performance metrics
    """

    def __init__(self, session_id: Optional[str] = None):
        self.session_id = session_id
        self.registry = get_registry_manager()

        # Session-level tracking (for limits enforcement)
        self._agent_invocation_counts: Dict[str, int] = {}
        self._tool_invocation_count = 0
        self._llm_call_count = 0
        self._policy_violations: List[PolicyViolation] = []
```

**Key Methods**:

#### 2.2.1 Agent Invocation Governance

```python
def check_agent_invocation(
    self,
    invoker_agent_id: str,
    target_agent_id: str
) -> EnforcementResult:
    """
    Check if agent invocation is allowed.

    Demonstrates: Runtime governance with audit trail.
    """
    # Check 1: Registry-based access control
    if not self.registry.is_agent_invocation_allowed(invoker_agent_id, target_agent_id):
        violation = PolicyViolation(
            violation_type=ViolationType.AGENT_INVOCATION_DENIED,
            agent_id=invoker_agent_id,
            target=target_agent_id,
            reason=f"Agent '{invoker_agent_id}' not permitted to invoke '{target_agent_id}' per governance policy",
            timestamp=datetime.utcnow().isoformat() + "Z",
            session_id=self.session_id
        )
        self._policy_violations.append(violation)

        return EnforcementResult(
            allowed=False,
            violation=violation
        )

    # Check 2: Duplicate invocation detection
    current_count = self._agent_invocation_counts.get(target_agent_id, 0)
    max_duplicates = 2  # Configurable from env or governance

    if current_count >= max_duplicates:
        violation = PolicyViolation(
            violation_type=ViolationType.MAX_INVOCATIONS_EXCEEDED,
            agent_id=invoker_agent_id,
            target=target_agent_id,
            reason=f"Agent '{target_agent_id}' already invoked {current_count} times (max: {max_duplicates})",
            timestamp=datetime.utcnow().isoformat() + "Z",
            session_id=self.session_id
        )
        self._policy_violations.append(violation)

        return EnforcementResult(
            allowed=False,
            violation=violation
        )

    # Passed - increment counter
    self._agent_invocation_counts[target_agent_id] = current_count + 1

    # Warning if approaching limit
    warning = None
    if current_count + 1 == max_duplicates:
        warning = f"Agent '{target_agent_id}' invoked {current_count + 1} times (limit: {max_duplicates})"

    return EnforcementResult(
        allowed=True,
        warning=warning
    )
```

#### 2.2.2 Tool Access Governance

```python
def check_tool_access(
    self,
    agent_id: str,
    tool_id: str
) -> EnforcementResult:
    """
    Check if agent can use tool.

    Demonstrates: Policy-driven tool access control.
    """
    # Check 1: Registry-based access control
    if not self.registry.is_tool_access_allowed(agent_id, tool_id):
        violation = PolicyViolation(
            violation_type=ViolationType.TOOL_ACCESS_DENIED,
            agent_id=agent_id,
            target=tool_id,
            reason=f"Agent '{agent_id}' not permitted to use tool '{tool_id}' per governance policy",
            timestamp=datetime.utcnow().isoformat() + "Z",
            session_id=self.session_id
        )
        self._policy_violations.append(violation)

        return EnforcementResult(
            allowed=False,
            violation=violation
        )

    # Check 2: Session-level tool invocation limit
    governance = self.registry.get_governance_policies()
    if governance:
        max_invocations = governance.policies.get("execution_constraints", {}).get(
            "max_tool_invocations_per_session", 50
        )

        if self._tool_invocation_count >= max_invocations:
            violation = PolicyViolation(
                violation_type=ViolationType.MAX_INVOCATIONS_EXCEEDED,
                agent_id=agent_id,
                target=tool_id,
                reason=f"Session tool invocation limit reached ({max_invocations})",
                timestamp=datetime.utcnow().isoformat() + "Z",
                session_id=self.session_id
            )
            self._policy_violations.append(violation)

            return EnforcementResult(
                allowed=False,
                violation=violation
            )

    # Passed - increment counter
    self._tool_invocation_count += 1

    return EnforcementResult(allowed=True)
```

#### 2.2.3 Audit & Reporting

```python
def get_violations(self) -> List[PolicyViolation]:
    """
    Get all policy violations for this session.

    Demonstrates: Audit trail for compliance.
    """
    return self._policy_violations.copy()

def get_enforcement_stats(self) -> Dict[str, Any]:
    """
    Get enforcement statistics for observability.

    Demonstrates: Metrics for monitoring governance effectiveness.
    """
    return {
        "session_id": self.session_id,
        "total_violations": len(self._policy_violations),
        "violations_by_type": self._count_violations_by_type(),
        "agent_invocation_counts": self._agent_invocation_counts.copy(),
        "tool_invocation_count": self._tool_invocation_count,
        "llm_call_count": self._llm_call_count
    }
```

**Scalability Patterns**:
- **Stateless enforcement**: No persistent state, can run on any instance
- **Audit trail**: All violations logged for compliance
- **Multi-layer checks**: Registry + session limits
- **Observability**: Stats for monitoring

---

### 2.3 Context Compiler

**File**: `backend/orchestrator/app/services/context_compiler.py`

**Purpose**: Token-aware context management to prevent LLM cost explosion.

**Architecture**:

```python
class ContextCompiler:
    """
    Production-grade context compiler.

    Prevents context bloat (major LLM cost driver at scale).

    Demonstrates:
    - Agent-specific context scoping
    - Token budget enforcement
    - Prior output dependency resolution
    - Context size optimization
    """

    def __init__(self):
        self.registry = get_registry_manager()
```

**Key Method**:

#### 2.3.1 Compile Context for Agent

```python
def compile_for_agent(
    self,
    agent_id: str,
    original_input: Optional[Dict[str, Any]],
    prior_outputs: Optional[Dict[str, Dict[str, Any]]],
    observations: Optional[List[Dict[str, Any]]]
) -> CompiledContext:
    """
    Compile scoped context for an agent.

    Demonstrates:
    - Agent-specific context requirements
    - Token budget enforcement
    - Selective prior output inclusion
    - Observation management
    """
    agent = self.registry.get_agent(agent_id)
    if not agent:
        raise ValueError(f"Agent '{agent_id}' not found")

    # Get context requirements
    context_reqs = agent.context_requirements
    max_tokens = context_reqs.get("max_context_tokens", 10000)
    requires_prior = context_reqs.get("requires_prior_outputs", [])

    # Budget allocation strategy:
    # - Original input: 30% of budget
    # - Prior outputs: 50% of budget
    # - Observations: 20% of budget
    budget_original = int(max_tokens * 0.3)
    budget_prior = int(max_tokens * 0.5)
    budget_obs = int(max_tokens * 0.2)

    # Build scoped context
    context = CompiledContext(
        agent_id=agent_id,
        max_tokens=max_tokens
    )

    # Include original input (if needed)
    if original_input:
        context.original_input = self._truncate_if_needed(
            original_input,
            budget_original
        )
        context.token_count += self._estimate_tokens(context.original_input)

    # Include required prior outputs
    if prior_outputs and requires_prior:
        context.prior_outputs = self._select_prior_outputs(
            prior_outputs,
            requires_prior,
            budget_prior
        )
        context.token_count += self._estimate_tokens(context.prior_outputs)

    # Include observations
    if observations:
        context.observations = self._select_observations(
            observations,
            budget_obs
        )
        context.token_count += self._estimate_tokens(context.observations)

    return context
```

**Token Estimation**:

```python
def _estimate_tokens(self, data: Any) -> int:
    """
    Estimate token count for data.

    Simple heuristic: 1 token ≈ 4 characters.
    Production would use tiktoken.
    """
    text = json.dumps(data)
    return len(text) // 4
```

**Prior Output Selection**:

```python
def _select_prior_outputs(
    self,
    prior_outputs: Dict[str, Dict[str, Any]],
    requires_prior: List[str],
    budget: int
) -> Dict[str, Dict[str, Any]]:
    """
    Select and truncate prior outputs based on requirements and budget.

    Demonstrates: Dependency-aware context compilation.
    """
    selected = {}
    tokens_used = 0

    # Only include required outputs
    for agent_id in requires_prior:
        if agent_id in prior_outputs and tokens_used < budget:
            output = prior_outputs[agent_id]
            output_tokens = self._estimate_tokens(output)

            # Check if fits in budget
            if tokens_used + output_tokens <= budget:
                selected[agent_id] = output
                tokens_used += output_tokens
            else:
                # Truncate to fit
                remaining_budget = budget - tokens_used
                selected[agent_id] = self._truncate_if_needed(
                    output,
                    remaining_budget
                )
                break

    return selected
```

**Observation Selection**:

```python
def _select_observations(
    self,
    observations: List[Dict[str, Any]],
    budget: int
) -> List[Dict[str, Any]]:
    """
    Select most recent observations that fit in budget.

    Demonstrates: Recency-biased observation selection.
    """
    selected = []
    tokens_used = 0

    # Include most recent observations first (reverse order)
    for obs in reversed(observations):
        obs_tokens = self._estimate_tokens(obs)

        if tokens_used + obs_tokens <= budget:
            selected.insert(0, obs)  # Maintain chronological order
            tokens_used += obs_tokens
        else:
            break

    return selected
```

**Scalability Patterns**:
- **Token-aware**: Prevents context explosion
- **Budget allocation**: Strategic distribution (30/50/20)
- **Selective inclusion**: Only required dependencies
- **Recency-bias**: Most recent observations prioritized
- **Production-ready**: Easy to swap heuristic for tiktoken

---

### 2.4 Agent ReAct Loop Controller

**File**: `backend/orchestrator/app/services/agent_react_loop.py`

**Purpose**: Production ReAct pattern engine for worker agents.

**Architecture**:

```python
class AgentReActLoopController:
    """
    Production-grade ReAct loop controller for worker agents.

    Manages the Reasoning + Acting loop:
    1. Compile context
    2. Call LLM for reasoning
    3. Parse action decision
    4. If use_tools: invoke tools, add to observations, loop
    5. If final_output: validate and return
    6. Enforce iteration limits and governance

    Demonstrates:
    - Bounded execution (max iterations)
    - Governance enforcement
    - Full observability (JSONL logging)
    - Error resilience
    """

    def __init__(
        self,
        session_id: str,
        agent_id: str,
        llm_client: Optional[Any] = None,
        tools_client: Optional[Any] = None
    ):
        self.session_id = session_id
        self.agent_id = agent_id
        self.llm_client = llm_client
        self.tools_client = tools_client

        # Dependencies
        self.registry = get_registry_manager()
        self.governance = create_governance_enforcer(session_id)
        self.context_compiler = create_context_compiler()
        self.storage = get_session_writer()

        # Agent metadata
        self.agent = self.registry.get_agent(agent_id)
        if not self.agent:
            raise ValueError(f"Agent '{agent_id}' not found in registry")

        # State
        self.observations: List[Dict[str, Any]] = []
        self.iteration = 0
        self.tool_calls_made = 0
```

**Key Methods**:

#### 2.4.1 Execute ReAct Loop

```python
def execute(
    self,
    context_data: Dict[str, Any],
    original_input: Optional[Dict[str, Any]] = None,
    prior_outputs: Optional[Dict[str, Dict[str, Any]]] = None
) -> AgentReActResult:
    """
    Execute ReAct loop for agent.

    Demonstrates:
    - Complete ReAct pattern
    - Bounded iteration
    - Full observability
    """
    self._log_event("agent_started", {
        "agent_id": self.agent_id,
        "max_iterations": self.agent.max_iterations
    })

    warnings = []

    try:
        # ReAct Loop
        while self.iteration < self.agent.max_iterations:
            self.iteration += 1

            # Check iteration limit governance
            limit_check = self.governance.check_iteration_limit(
                self.agent_id,
                self.iteration
            )
            if not limit_check.allowed:
                self._log_event("iteration_limit_exceeded", {
                    "iteration": self.iteration,
                    "max": self.agent.max_iterations
                })
                warnings.append(f"Max iterations ({self.agent.max_iterations}) reached")
                break

            # Step 1: Compile context
            compiled_context = self._compile_context(
                original_input,
                prior_outputs
            )

            # Step 2: Call LLM for reasoning
            reasoning = self._call_llm_for_reasoning(compiled_context)

            # Log reasoning
            self._log_event("agent_reasoning", {
                "agent_id": self.agent_id,
                "iteration": self.iteration,
                "reasoning": reasoning.reasoning,
                "action_type": reasoning.action.type.value
            })

            # Step 3: Handle action
            if reasoning.action.type == ActionType.USE_TOOLS:
                # Execute tools and add observations
                success = self._execute_tools(reasoning.action.tool_requests or [])
                if not success:
                    warnings.append("Some tool executions failed")
                # Continue loop

            elif reasoning.action.type == ActionType.FINAL_OUTPUT:
                # Validate output
                output = reasoning.action.output
                if self._validate_output(output):
                    self._log_event("agent_completed", {
                        "agent_id": self.agent_id,
                        "iterations_used": self.iteration,
                        "tool_calls_made": self.tool_calls_made
                    })

                    return AgentReActResult(
                        agent_id=self.agent_id,
                        status="completed",
                        output=output,
                        iterations_used=self.iteration,
                        tool_calls_made=self.tool_calls_made,
                        warnings=warnings
                    )
                else:
                    warnings.append("Output validation failed, continuing loop")

        # Loop exhausted without final_output
        self._log_event("agent_incomplete", {
            "agent_id": self.agent_id,
            "iterations_used": self.iteration,
            "reason": "max_iterations_reached"
        })

        # Return best available output (partial)
        partial_output = self._extract_partial_output()

        return AgentReActResult(
            agent_id=self.agent_id,
            status="incomplete",
            output=partial_output,
            iterations_used=self.iteration,
            tool_calls_made=self.tool_calls_made,
            warnings=warnings + ["Agent reached max iterations without completing"]
        )

    except Exception as e:
        self._log_event("agent_error", {
            "agent_id": self.agent_id,
            "error": str(e),
            "iteration": self.iteration
        })

        return AgentReActResult(
            agent_id=self.agent_id,
            status="error",
            error=str(e),
            iterations_used=self.iteration,
            tool_calls_made=self.tool_calls_made
        )
```

#### 2.4.2 Execute Tools

```python
def _execute_tools(
    self,
    tool_requests: List[ToolRequest]
) -> bool:
    """
    Execute requested tools and add results to observations.

    Demonstrates:
    - Governance enforcement
    - Error handling
    - Observation accumulation
    """
    all_success = True

    for tool_req in tool_requests:
        # Governance check
        access_check = self.governance.check_tool_access(
            self.agent_id,
            tool_req.tool_id
        )

        if not access_check.allowed:
            self._log_event("tool_denied", {
                "agent_id": self.agent_id,
                "tool_id": tool_req.tool_id,
                "reason": access_check.violation.reason if access_check.violation else "unknown"
            })
            all_success = False
            continue

        # Execute tool
        tool_result = self._invoke_tool(tool_req.tool_id, tool_req.parameters)

        if tool_result.get("success"):
            # Add to observations
            self.observations.append({
                "iteration": self.iteration,
                "tool_id": tool_req.tool_id,
                "parameters": tool_req.parameters,
                "result": tool_result.get("result"),
                "timestamp": datetime.utcnow().isoformat() + "Z"
            })

            self._log_event("tool_invocation", {
                "agent_id": self.agent_id,
                "iteration": self.iteration,
                "tool_id": tool_req.tool_id,
                "success": True
            })

            self.tool_calls_made += 1
        else:
            self._log_event("tool_error", {
                "agent_id": self.agent_id,
                "tool_id": tool_req.tool_id,
                "error": tool_result.get("error")
            })
            all_success = False

    return all_success
```

**Scalability Patterns**:
- **Bounded execution**: Hard iteration limits
- **Governance integration**: Every tool checked
- **Full observability**: Every event logged
- **Error resilience**: Graceful degradation
- **Partial outputs**: Best-effort completion

---

### 2.5 Orchestrator Runner

**File**: `backend/orchestrator/app/services/orchestrator_runner.py`

**Purpose**: Meta-agent ReAct loop that discovers and invokes worker agents.

**Architecture**:

```python
class OrchestratorRunner:
    """
    Production-grade orchestrator ReAct loop controller.

    Manages meta-agent that discovers and invokes worker agents dynamically.

    Key differences from AgentReActLoopController:
    - Actions are "invoke_agents" instead of "use_tools"
    - Manages workflow state and completion criteria
    - Produces final evidence map
    - Handles advisory workflow guidance

    Demonstrates:
    - Dynamic agent discovery from registry
    - Advisory workflow mode (adapt based on state)
    - Multi-tier completion logic
    - Agent invocation governance
    - Evidence map compilation
    """

    def __init__(
        self,
        session_id: str,
        workflow_id: str,
        llm_client: Optional[Any] = None
    ):
        self.session_id = session_id
        self.workflow_id = workflow_id
        self.llm_client = llm_client

        # Dependencies
        self.registry = get_registry_manager()
        self.governance = create_governance_enforcer(session_id)
        self.context_compiler = create_context_compiler()
        self.storage = get_session_writer()
        self.config = get_config()

        # Load workflow and orchestrator agent
        self.workflow = self.registry.get_workflow(workflow_id)
        self.orchestrator_agent = self.registry.get_agent("orchestrator_agent")

        # State
        self.observations: List[Dict[str, Any]] = []
        self.iteration = 0
        self.agent_invocations: Dict[str, int] = {}
        self.prior_outputs: Dict[str, Dict[str, Any]] = {}
        self.agents_executed_order: List[str] = []
```

**Key Methods**:

#### 2.5.1 Execute Orchestrator Loop

```python
def execute(
    self,
    original_input: Dict[str, Any]
) -> OrchestratorResult:
    """
    Execute orchestrator ReAct loop.

    Demonstrates:
    - Dynamic agent discovery and selection
    - Advisory workflow guidance
    - Multi-tier completion detection
    - Full observability
    """
    self._log_event("orchestrator_started", {
        "workflow_id": self.workflow_id,
        "workflow_mode": self.workflow.mode,
        "max_iterations": self.orchestrator_agent.max_iterations,
        "workflow_goal": self.workflow.goal
    })

    warnings = []

    try:
        # Orchestrator ReAct Loop
        while self.iteration < self.orchestrator_agent.max_iterations:
            self.iteration += 1

            # Step 1: Compile context for orchestrator
            compiled_context = self._compile_orchestrator_context(original_input)

            # Step 2: Call LLM for orchestrator reasoning
            reasoning = self._call_llm_for_orchestrator_reasoning(compiled_context)

            # Log orchestrator reasoning
            self._log_event("orchestrator_reasoning", {
                "iteration": self.iteration,
                "reasoning": reasoning.reasoning,
                "workflow_state_assessment": reasoning.workflow_state_assessment,
                "action_type": reasoning.action.type.value
            })

            # Step 3: Handle orchestrator action
            if reasoning.action.type == OrchestratorActionType.INVOKE_AGENTS:
                # Execute agent invocations
                success = self._execute_agent_invocations(
                    reasoning.action.agent_requests or [],
                    original_input
                )
                if not success:
                    warnings.append("Some agent invocations failed")
                # Continue loop

            elif reasoning.action.type == OrchestratorActionType.WORKFLOW_COMPLETE:
                # Tier 1: LLM explicit completion signal
                evidence_map = reasoning.action.evidence_map

                # Tier 2: Validate against completion criteria
                validation_result = self._validate_completion_criteria(evidence_map)

                if validation_result["valid"]:
                    self._log_event("orchestrator_completed", {
                        "completion_reason": "all_objectives_achieved",
                        "total_iterations": self.iteration,
                        "agents_executed": self.agents_executed_order
                    })

                    return OrchestratorResult(
                        session_id=self.session_id,
                        workflow_id=self.workflow_id,
                        status="completed",
                        completion_reason="all_objectives_achieved",
                        evidence_map=evidence_map,
                        agents_executed=self.agents_executed_order,
                        total_iterations=self.iteration,
                        total_agent_invocations=sum(self.agent_invocations.values()),
                        warnings=warnings
                    )
                else:
                    # Validation failed - continue loop
                    warnings.append(f"Completion validation failed: {validation_result['reason']}")

        # Tier 3: Forced completion (max iterations reached)
        evidence_map = self._build_evidence_map()

        return OrchestratorResult(
            session_id=self.session_id,
            workflow_id=self.workflow_id,
            status="incomplete",
            completion_reason="max_iterations_reached",
            evidence_map=evidence_map,
            agents_executed=self.agents_executed_order,
            total_iterations=self.iteration,
            total_agent_invocations=sum(self.agent_invocations.values()),
            warnings=warnings + ["Orchestrator reached max iterations"]
        )

    except Exception as e:
        return OrchestratorResult(
            session_id=self.session_id,
            workflow_id=self.workflow_id,
            status="error",
            completion_reason="error",
            error=str(e),
            agents_executed=self.agents_executed_order,
            total_iterations=self.iteration
        )
```

#### 2.5.2 Execute Agent Invocations

```python
def _execute_agent_invocations(
    self,
    agent_requests: List[AgentInvocationRequest],
    original_input: Dict[str, Any]
) -> bool:
    """
    Execute requested agent invocations.

    Demonstrates:
    - Agent invocation governance
    - Agent ReAct loop coordination
    - Output accumulation
    - Observation tracking
    """
    all_success = True

    for agent_req in agent_requests:
        # Governance check
        access_check = self.governance.check_agent_invocation(
            "orchestrator_agent",
            agent_req.agent_id
        )

        if not access_check.allowed:
            self._log_event("agent_invocation_denied", {
                "agent_id": agent_req.agent_id,
                "reason": access_check.violation.reason
            })
            all_success = False
            continue

        # Execute agent via AgentReActLoopController
        agent_result = self._invoke_agent(
            agent_req.agent_id,
            original_input
        )

        if agent_result.status == "completed":
            # Add to prior_outputs for subsequent agents
            self.prior_outputs[agent_req.agent_id] = agent_result.output

            # Track execution
            self.agents_executed_order.append(agent_req.agent_id)
            self.agent_invocations[agent_req.agent_id] = \
                self.agent_invocations.get(agent_req.agent_id, 0) + 1

            # Add to observations for orchestrator's next iteration
            self.observations.append({
                "iteration": self.iteration,
                "agent_id": agent_req.agent_id,
                "reasoning": agent_req.reasoning,
                "result": agent_result.output,
                "iterations_used": agent_result.iterations_used,
                "tool_calls_made": agent_result.tool_calls_made,
                "timestamp": datetime.utcnow().isoformat() + "Z"
            })

            self._log_event("agent_invocation_completed", {
                "orchestrator_iteration": self.iteration,
                "agent_id": agent_req.agent_id,
                "agent_iterations": agent_result.iterations_used
            })
        else:
            all_success = False

    return all_success

def _invoke_agent(
    self,
    agent_id: str,
    original_input: Dict[str, Any]
) -> AgentReActResult:
    """
    Invoke agent via AgentReActLoopController.

    Demonstrates: Composition of ReAct loops (meta-loop → agent loop).
    """
    # Create agent ReAct loop controller
    agent_loop = create_agent_react_loop(
        session_id=self.session_id,
        agent_id=agent_id,
        llm_client=self.llm_client
    )

    # Execute agent's ReAct loop
    result = agent_loop.execute(
        context_data={},
        original_input=original_input,
        prior_outputs=self.prior_outputs
    )

    return result
```

#### 2.5.3 Multi-Tier Completion

```python
def _validate_completion_criteria(
    self,
    evidence_map: Optional[Dict[str, Any]]
) -> Dict[str, Any]:
    """
    Validate workflow completion criteria (Tier 2 completion).

    Checks:
    - Required agents executed
    - Required outputs present
    - Minimum agent count
    - Evidence map structure
    """
    if not self.workflow.completion_criteria:
        return {"valid": True}

    criteria = self.workflow.completion_criteria

    # Check required agents executed
    required_agents = criteria.get("required_agents_executed", [])
    for agent_id in required_agents:
        if agent_id not in self.agents_executed_order:
            return {
                "valid": False,
                "reason": f"Required agent '{agent_id}' not executed"
            }

    # Check minimum agent count
    min_agents = criteria.get("min_agents_executed", 0)
    if len(self.agents_executed_order) < min_agents:
        return {
            "valid": False,
            "reason": f"Only {len(self.agents_executed_order)} agents executed (min: {min_agents})"
        }

    # Check required outputs present
    required_outputs = criteria.get("required_outputs", [])
    if not evidence_map:
        if required_outputs:
            return {
                "valid": False,
                "reason": "Evidence map missing but required outputs specified"
            }
    else:
        for output_key in required_outputs:
            if output_key not in evidence_map:
                return {
                    "valid": False,
                    "reason": f"Required output '{output_key}' missing from evidence map"
                }

    return {"valid": True}

def _build_evidence_map(self) -> Dict[str, Any]:
    """
    Build evidence map from all agent outputs (Tier 3 fallback).

    Demonstrates: Evidence compilation for explainability.
    """
    # If explainability agent ran, use its output
    if "explainability_agent" in self.prior_outputs:
        return self.prior_outputs["explainability_agent"]

    # Otherwise, compile from available outputs
    evidence_map = {
        "decision": {},
        "supporting_evidence": [],
        "assumptions": [],
        "limitations": ["Incomplete workflow - evidence map auto-generated"],
        "agent_chain": self.agents_executed_order.copy()
    }

    # Extract recommendation if available
    if "recommendation_agent" in self.prior_outputs:
        rec_output = self.prior_outputs["recommendation_agent"]
        evidence_map["decision"] = {
            "outcome": rec_output.get("recommended_action"),
            "confidence": rec_output.get("confidence")
        }

    # Compile supporting evidence from all agents
    for agent_id, output in self.prior_outputs.items():
        evidence_map["supporting_evidence"].append({
            "source": agent_id,
            "evidence_type": "agent_output",
            "summary": str(output)[:200]
        })

    return evidence_map
```

**Scalability Patterns**:
- **Dynamic discovery**: Agents from registry, not hardcoded
- **Advisory workflow**: Adapts based on state
- **Multi-tier completion**: LLM signal → validation → forced
- **Composition**: Meta-loop coordinates agent loops
- **Evidence compilation**: Explainability built-in

---

## Phase 3: LLM Integration

**Goal**: Integrate multi-provider LLM support with real ReAct pattern implementation.

**Status**: ✅ Complete

### 3.1 Multi-Provider LLM Client

**File**: `backend/orchestrator/app/services/llm_client.py`

**Purpose**: Unified interface for OpenAI and Anthropic (Claude) with retry logic and observability.

**Architecture**:

```python
class BaseLLMClient(ABC):
    """
    Abstract base class for LLM clients.

    Demonstrates: Provider-agnostic interface pattern.
    """

    def __init__(
        self,
        model_profile: ModelProfile,
        session_id: Optional[str] = None
    ):
        self.model_profile = model_profile
        self.session_id = session_id
        self.config = get_config()
        self.storage = get_session_writer() if session_id else None

        # Metrics
        self.total_calls = 0
        self.total_tokens = 0
        self.failed_calls = 0

    @abstractmethod
    def call(
        self,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> LLMResponse:
        """Call LLM with messages."""
        pass

    def _log_llm_call(
        self,
        messages: List[Dict[str, str]],
        response: Optional[LLMResponse],
        error: Optional[str] = None,
        attempt: int = 1
    ) -> None:
        """Log LLM call for observability."""
        if not self.storage or not self.session_id:
            return

        event = {
            "event_type": "llm_call",
            "session_id": self.session_id,
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "provider": self.model_profile.provider,
            "model": self.model_profile.model_name,
            "attempt": attempt,
            "message_count": len(messages),
            "success": response is not None,
            "error": error
        }

        if response:
            event.update({
                "tokens_used": response.tokens_used,
                "latency_ms": response.latency_ms,
                "finish_reason": response.finish_reason
            })

        self.storage.write_event(self.session_id, event)
```

#### 3.1.1 OpenAI Client

```python
class OpenAIClient(BaseLLMClient):
    """
    OpenAI provider client.

    Demonstrates: Provider-specific implementation with retry and timeout.
    """

    def __init__(
        self,
        model_profile: ModelProfile,
        session_id: Optional[str] = None
    ):
        super().__init__(model_profile, session_id)

        from openai import OpenAI
        self.client = OpenAI(api_key=self.config.openai_api_key)

    def call(
        self,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> LLMResponse:
        """
        Call OpenAI API with retry logic.

        Demonstrates: Exponential backoff retry pattern.
        """
        retry_policy = self.model_profile.retry_policy
        max_retries = retry_policy.get("max_retries", 3)
        backoff_multiplier = retry_policy.get("backoff_multiplier", 2)
        initial_delay = retry_policy.get("initial_delay_ms", 1000) / 1000.0

        last_error = None

        for attempt in range(1, max_retries + 1):
            try:
                start_time = time.time()

                # Build request parameters
                request_params = {
                    "model": self.model_profile.model_name,
                    "messages": messages,
                    "temperature": self.model_profile.parameters.get("temperature", 0.3),
                    "max_tokens": self.model_profile.parameters.get("max_tokens", 2000),
                    "timeout": self.model_profile.timeout_seconds
                }

                # Enable JSON mode if supported
                if self.model_profile.json_mode:
                    request_params["response_format"] = {"type": "json_object"}

                # Override with any kwargs
                request_params.update(kwargs)

                # Call OpenAI API
                response = self.client.chat.completions.create(**request_params)

                latency_ms = int((time.time() - start_time) * 1000)

                # Extract response
                llm_response = LLMResponse(
                    content=response.choices[0].message.content,
                    model=response.model,
                    provider="openai",
                    tokens_used={
                        "prompt": response.usage.prompt_tokens,
                        "completion": response.usage.completion_tokens,
                        "total": response.usage.total_tokens
                    },
                    latency_ms=latency_ms,
                    finish_reason=response.choices[0].finish_reason
                )

                # Update metrics
                self.total_calls += 1
                self.total_tokens += llm_response.tokens_used["total"]

                # Log success
                self._log_llm_call(messages, llm_response, attempt=attempt)

                return llm_response

            except Exception as e:
                last_error = str(e)
                self.failed_calls += 1

                # Log failure
                self._log_llm_call(messages, None, error=last_error, attempt=attempt)

                if attempt < max_retries:
                    # Calculate delay with exponential backoff
                    delay = initial_delay * (backoff_multiplier ** (attempt - 1))
                    time.sleep(delay)
                else:
                    # Final attempt failed
                    raise RuntimeError(
                        f"OpenAI API call failed after {max_retries} attempts. Last error: {last_error}"
                    )
```

#### 3.1.2 Claude Client

```python
class ClaudeClient(BaseLLMClient):
    """
    Anthropic (Claude) provider client.

    Demonstrates: Multi-provider support with different API patterns.
    """

    def __init__(
        self,
        model_profile: ModelProfile,
        session_id: Optional[str] = None
    ):
        super().__init__(model_profile, session_id)

        from anthropic import Anthropic
        self.client = Anthropic(api_key=self.config.anthropic_api_key)

    def call(
        self,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> LLMResponse:
        """
        Call Anthropic API with retry logic.

        Note: Claude doesn't have native JSON mode, so we use prompt engineering.
        """
        retry_policy = self.model_profile.retry_policy
        max_retries = retry_policy.get("max_retries", 3)
        backoff_multiplier = retry_policy.get("backoff_multiplier", 2)
        initial_delay = retry_policy.get("initial_delay_ms", 1000) / 1000.0

        last_error = None

        for attempt in range(1, max_retries + 1):
            try:
                start_time = time.time()

                # Claude API expects system message separate from messages
                system_message = None
                chat_messages = []

                for msg in messages:
                    if msg["role"] == "system":
                        system_message = msg["content"]
                    else:
                        chat_messages.append({
                            "role": msg["role"],
                            "content": msg["content"]
                        })

                # Build request parameters
                request_params = {
                    "model": self.model_profile.model_name,
                    "messages": chat_messages,
                    "temperature": self.model_profile.parameters.get("temperature", 0.3),
                    "max_tokens": self.model_profile.parameters.get("max_tokens", 4000),
                    "timeout": self.model_profile.timeout_seconds
                }

                # Add system message if present
                if system_message:
                    request_params["system"] = system_message

                # Override with any kwargs
                request_params.update(kwargs)

                # Call Anthropic API
                response = self.client.messages.create(**request_params)

                latency_ms = int((time.time() - start_time) * 1000)

                # Extract response
                llm_response = LLMResponse(
                    content=response.content[0].text,
                    model=response.model,
                    provider="anthropic",
                    tokens_used={
                        "prompt": response.usage.input_tokens,
                        "completion": response.usage.output_tokens,
                        "total": response.usage.input_tokens + response.usage.output_tokens
                    },
                    latency_ms=latency_ms,
                    finish_reason=response.stop_reason
                )

                # Update metrics
                self.total_calls += 1
                self.total_tokens += llm_response.tokens_used["total"]

                # Log success
                self._log_llm_call(messages, llm_response, attempt=attempt)

                return llm_response

            except Exception as e:
                last_error = str(e)
                self.failed_calls += 1

                # Log failure
                self._log_llm_call(messages, None, error=last_error, attempt=attempt)

                if attempt < max_retries:
                    delay = initial_delay * (backoff_multiplier ** (attempt - 1))
                    time.sleep(delay)
                else:
                    raise RuntimeError(
                        f"Anthropic API call failed after {max_retries} attempts. Last error: {last_error}"
                    )
```

#### 3.1.3 Factory Function

```python
def create_llm_client(
    model_profile: ModelProfile,
    session_id: Optional[str] = None
) -> BaseLLMClient:
    """
    Factory function to create appropriate LLM client based on provider.

    Demonstrates: Factory pattern for multi-provider routing.
    """
    provider = model_profile.provider.lower()

    if provider == "openai":
        return OpenAIClient(model_profile, session_id)
    elif provider == "anthropic":
        return ClaudeClient(model_profile, session_id)
    else:
        raise ValueError(
            f"Unknown LLM provider: {provider}. Supported: openai, anthropic"
        )
```

**Scalability Patterns**:
- **Provider abstraction**: Single interface, multiple implementations
- **Retry with backoff**: Resilience against transient failures
- **Timeout handling**: Prevents hanging calls
- **Token tracking**: Cost monitoring
- **Full logging**: Observability for debugging
- **Metrics**: Performance monitoring

---

### 3.2 ReAct System Prompts

**File**: `backend/orchestrator/app/prompts/react_prompts.py`

**Purpose**: Production-grade prompt engineering for dynamic agent and tool discovery.

#### 3.2.1 Orchestrator Prompt

```python
def build_orchestrator_prompt(
    agent_name: str,
    agent_description: str,
    workflow_goal: str,
    available_agents: List[Dict[str, Any]],
    workflow_state: Dict[str, Any],
    prior_outputs: Dict[str, Dict[str, Any]],
    observations: List[Dict[str, Any]]
) -> List[Dict[str, str]]:
    """
    Build ReAct prompt for orchestrator agent.

    Demonstrates: Dynamic agent discovery and adaptive workflow execution.
    """

    # Format available agents catalog
    agents_catalog = []
    for agent in available_agents:
        agents_catalog.append({
            "agent_id": agent["agent_id"],
            "name": agent["name"],
            "description": agent["description"],
            "capabilities": agent["capabilities"],
            "required_prior_outputs": agent.get("required_prior_outputs", [])
        })

    agents_catalog_json = json.dumps(agents_catalog, indent=2)
    workflow_state_json = json.dumps(workflow_state, indent=2)
    prior_outputs_json = json.dumps(prior_outputs, indent=2) if prior_outputs else "{}"
    observations_json = json.dumps(observations, indent=2) if observations else "[]"

    system_prompt = f"""You are {agent_name}, a meta-agent orchestrator for insurance claims processing.

## Your Role
{agent_description}

## Workflow Goal
{workflow_goal}

## Available Agents
You can discover and invoke the following agents dynamically based on workflow state:

{agents_catalog_json}

## Current Workflow State
{workflow_state_json}

## Prior Agent Outputs
{prior_outputs_json}

## Your Task
You must reason about:
1. What has been accomplished (agents_executed in workflow state)
2. What is still needed to achieve the workflow goal
3. Which agent(s) should be invoked next based on:
   - Their capabilities and what they can contribute
   - Whether their required_prior_outputs have been satisfied
   - Whether they've already been executed
4. When all objectives are met and you have sufficient information for evidence map

## Output Format
You MUST respond with valid JSON:

{{
  "reasoning": "Your step-by-step reasoning",
  "workflow_state_assessment": "Summary of what's done and what's missing",
  "action": {{
    "type": "invoke_agents" | "workflow_complete",
    "agent_requests": [  // Only if type is "invoke_agents"
      {{"agent_id": "...", "reasoning": "..."}}
    ],
    "evidence_map": {{...}}  // Only if type is "workflow_complete"
  }}
}}

## Observations from Previous Iterations
{observations_json}

Now reason about the workflow state and decide the next action."""

    return [{"role": "system", "content": system_prompt}]
```

**Key Features**:
- **Agent catalog injection**: LLM sees all available agents with descriptions
- **Workflow state**: LLM knows what's been done and what's missing
- **Prior outputs**: LLM can see results from executed agents
- **Dependency awareness**: LLM knows required_prior_outputs for each agent
- **Structured output**: Forces JSON format for reliable parsing

#### 3.2.2 Worker Agent Prompt

```python
def build_worker_agent_prompt(
    agent_name: str,
    agent_description: str,
    agent_capabilities: List[str],
    available_tools: List[Dict[str, Any]],
    working_context: Dict[str, Any],
    observations: List[Dict[str, Any]]
) -> List[Dict[str, str]]:
    """
    Build ReAct prompt for worker agent.

    Demonstrates: Tool discovery and usage via ReAct pattern.
    """

    # Format available tools catalog
    tools_catalog = []
    for tool in available_tools:
        tools_catalog.append({
            "tool_id": tool["tool_id"],
            "name": tool["name"],
            "description": tool["description"],
            "input_schema": tool.get("input_schema", {})
        })

    tools_catalog_json = json.dumps(tools_catalog, indent=2)
    working_context_json = json.dumps(working_context, indent=2)
    observations_json = json.dumps(observations, indent=2) if observations else "[]"
    capabilities_str = "\n".join(f"- {cap}" for cap in agent_capabilities)

    system_prompt = f"""You are {agent_name}, a specialized agent for insurance claims processing.

## Your Role
{agent_description}

## Your Capabilities
{capabilities_str}

## Available Tools
{tools_catalog_json}

## Working Context
{working_context_json}

## Your Task
1. Analyze the working context based on your capabilities
2. Decide which tools you need to invoke
3. Use tools iteratively - review results and call more tools if needed
4. When you have sufficient information, produce final output

## Output Format
You MUST respond with valid JSON:

{{
  "reasoning": "Your step-by-step reasoning",
  "action": {{
    "type": "use_tools" | "final_output",
    "tool_requests": [  // Only if type is "use_tools"
      {{"tool_id": "...", "parameters": {{...}}}}
    ],
    "output": {{...}}  // Only if type is "final_output"
  }}
}}

## Tool Invocation Observations
{observations_json}

Now reason about the task and decide your next action."""

    return [{"role": "system", "content": system_prompt}]
```

**Key Features**:
- **Tool catalog injection**: LLM sees tools with descriptions and schemas
- **Working context**: Claim data + prior agent outputs
- **Capabilities focus**: LLM knows what it's supposed to do
- **Iterative usage**: Can call tools multiple times
- **Structured output**: JSON format for reliable parsing

---

### 3.3 Response Parser

**File**: `backend/orchestrator/app/services/response_parser.py`

**Purpose**: Robust JSON parsing with fallbacks for production reliability.

**Architecture**:

```python
def extract_json_from_response(content: str) -> str:
    """
    Extract JSON from LLM response.

    Handles cases where LLM wraps JSON in markdown or adds extra text.

    Demonstrates: Robust parsing with multiple extraction strategies.
    """
    # Strategy 1: Try to find JSON in markdown code block
    json_block_pattern = r'```(?:json)?\s*(\{.*?\})\s*```'
    match = re.search(json_block_pattern, content, re.DOTALL)
    if match:
        return match.group(1)

    # Strategy 2: Try to find JSON object directly
    json_object_pattern = r'\{.*\}'
    match = re.search(json_object_pattern, content, re.DOTALL)
    if match:
        return match.group(0)

    # Strategy 3: Content itself might be JSON
    return content.strip()
```

#### 3.3.1 Worker Agent Response Parsing

```python
def parse_worker_agent_response(
    response_content: str,
    agent_id: str
) -> AgentReasoning:
    """
    Parse worker agent LLM response into AgentReasoning.

    Demonstrates: Structured response parsing with validation.
    """
    try:
        # Extract JSON
        json_str = extract_json_from_response(response_content)

        # Parse JSON
        data = json.loads(json_str)

        # Validate required fields
        if "reasoning" not in data:
            raise ResponseParseError("Missing required field: reasoning")

        if "action" not in data:
            raise ResponseParseError("Missing required field: action")

        action_data = data["action"]

        if "type" not in action_data:
            raise ResponseParseError("Missing required field: action.type")

        # Parse action type
        action_type_str = action_data["type"]
        try:
            action_type = ActionType(action_type_str)
        except ValueError:
            raise ResponseParseError(
                f"Invalid action type: {action_type_str}"
            )

        # Parse based on action type
        if action_type == ActionType.USE_TOOLS:
            if "tool_requests" not in action_data:
                raise ResponseParseError("use_tools requires tool_requests field")

            tool_requests = []
            for tool_req_data in action_data["tool_requests"]:
                if "tool_id" not in tool_req_data:
                    raise ResponseParseError("Tool request missing tool_id")

                tool_requests.append(ToolRequest(
                    tool_id=tool_req_data["tool_id"],
                    parameters=tool_req_data.get("parameters", {})
                ))

            action = AgentAction(
                type=ActionType.USE_TOOLS,
                tool_requests=tool_requests
            )

        elif action_type == ActionType.FINAL_OUTPUT:
            if "output" not in action_data:
                raise ResponseParseError("final_output requires output field")

            action = AgentAction(
                type=ActionType.FINAL_OUTPUT,
                output=action_data["output"]
            )

        # Build AgentReasoning
        return AgentReasoning(
            reasoning=data["reasoning"],
            action=action
        )

    except json.JSONDecodeError as e:
        raise ResponseParseError(f"Invalid JSON: {str(e)}")
    except Exception as e:
        raise ResponseParseError(f"Unexpected error: {str(e)}")
```

#### 3.3.2 Orchestrator Response Parsing

```python
def parse_orchestrator_response(
    response_content: str
) -> OrchestratorReasoning:
    """
    Parse orchestrator LLM response into OrchestratorReasoning.

    Similar structure to worker agent parsing but for orchestrator actions.
    """
    # Similar implementation for orchestrator-specific action types
    # (invoke_agents vs workflow_complete)
```

#### 3.3.3 Fallback Responses

```python
def create_fallback_worker_response(
    agent_id: str,
    error_message: str
) -> AgentReasoning:
    """
    Create fallback response when parsing fails.

    Demonstrates: Graceful degradation pattern.
    """
    return AgentReasoning(
        reasoning=f"[FALLBACK] Response parsing failed: {error_message}",
        action=AgentAction(
            type=ActionType.FINAL_OUTPUT,
            output={"error": "response_parse_failure", "details": error_message}
        )
    )

def create_fallback_orchestrator_response(
    error_message: str,
    executed_agents: list
) -> OrchestratorReasoning:
    """
    Create fallback response when orchestrator parsing fails.

    Demonstrates: Forced completion on parse failure.
    """
    return OrchestratorReasoning(
        reasoning=f"[FALLBACK] Response parsing failed: {error_message}",
        workflow_state_assessment=f"Agents executed: {executed_agents}. Forced completion.",
        action=OrchestratorAction(
            type=OrchestratorActionType.WORKFLOW_COMPLETE,
            evidence_map={
                "decision": {"error": "response_parse_failure"},
                "supporting_evidence": [],
                "assumptions": ["Response parsing failed"],
                "limitations": [error_message],
                "agent_chain": executed_agents
            }
        )
    )
```

**Scalability Patterns**:
- **Multi-strategy extraction**: Handles markdown, code blocks, plain JSON
- **Schema validation**: Ensures required fields present
- **Graceful degradation**: Fallback responses prevent crashes
- **Clear error messages**: Facilitates debugging

---

### 3.4 LLM Integration into ReAct Loops

#### 3.4.1 Worker Agent Integration

**File**: `backend/orchestrator/app/services/agent_react_loop.py:249-327`

```python
def _call_llm_for_reasoning(
    self,
    context: CompiledContext
) -> AgentReasoning:
    """Call LLM to get agent's reasoning and action."""
    from ..prompts.react_prompts import build_worker_agent_prompt
    from .response_parser import parse_worker_agent_response, create_fallback_worker_response
    from .llm_client import create_llm_client

    try:
        # Get available tools for this agent
        available_tools = self.registry.get_tools_for_agent(self.agent_id)
        tools_list = [
            {
                "tool_id": tool.tool_id,
                "name": tool.name,
                "description": tool.description,
                "input_schema": tool.input_schema
            }
            for tool in available_tools
        ]

        # Build ReAct prompt
        messages = build_worker_agent_prompt(
            agent_name=self.agent.name,
            agent_description=self.agent.description,
            agent_capabilities=self.agent.capabilities,
            available_tools=tools_list,
            working_context=context.dict(),
            observations=self.observations
        )

        # Get model profile
        model_profile = self.registry.get_model_profile(self.agent.model_profile_id)

        # Create LLM client for this agent's model
        llm_client = create_llm_client(model_profile, self.session_id)

        # Call LLM
        llm_response = llm_client.call(messages)

        # Parse response
        reasoning = parse_worker_agent_response(llm_response.content, self.agent_id)

        return reasoning

    except ResponseParseError as e:
        # Parsing failed - return fallback
        self._log_event("llm_response_parse_error", {
            "agent_id": self.agent_id,
            "error": str(e)
        })
        return create_fallback_worker_response(self.agent_id, str(e))

    except Exception as e:
        # LLM call failed - return error fallback
        self._log_event("llm_call_error", {
            "agent_id": self.agent_id,
            "error": str(e)
        })
        return create_fallback_worker_response(self.agent_id, f"LLM call failed: {str(e)}")
```

#### 3.4.2 Orchestrator Integration

**File**: `backend/orchestrator/app/services/orchestrator_runner.py:306-396`

```python
def _call_llm_for_orchestrator_reasoning(
    self,
    context: Dict[str, Any]
) -> OrchestratorReasoning:
    """Call LLM to get orchestrator's reasoning and action."""
    from ..prompts.react_prompts import build_orchestrator_prompt
    from .response_parser import parse_orchestrator_response, create_fallback_orchestrator_response
    from .llm_client import create_llm_client

    try:
        # Build orchestrator ReAct prompt
        messages = build_orchestrator_prompt(
            agent_name=self.orchestrator_agent.name,
            agent_description=self.orchestrator_agent.description,
            workflow_goal=self.workflow.goal,
            available_agents=context["available_agents"],
            workflow_state=context["workflow_state"],
            prior_outputs=self.prior_outputs,
            observations=self.observations
        )

        # Get model profile
        model_profile = self.registry.get_model_profile(
            self.orchestrator_agent.model_profile_id
        )

        # Create LLM client for orchestrator's model
        llm_client = create_llm_client(model_profile, self.session_id)

        # Call LLM
        llm_response = llm_client.call(messages)

        # Parse response
        reasoning = parse_orchestrator_response(llm_response.content)

        return reasoning

    except ResponseParseError as e:
        self._log_event("llm_response_parse_error", {"error": str(e)})
        return create_fallback_orchestrator_response(str(e), self.agents_executed_order)

    except Exception as e:
        self._log_event("llm_call_error", {"error": str(e)})
        return create_fallback_orchestrator_response(f"LLM call failed: {str(e)}", self.agents_executed_order)
```

**Integration Flow**:

```
Agent Registry (model_profile_id: "claude_sonnet_35")
    ↓
Get ModelProfile from Registry
    ↓
Factory creates ClaudeClient (or OpenAIClient)
    ↓
Build ReAct prompt with agent/tool catalog
    ↓
Call LLM via client (with retry, timeout)
    ↓
Parse JSON response
    ↓
Return AgentReasoning/OrchestratorReasoning
    ↓
Execute action (use_tools OR final_output / invoke_agents OR workflow_complete)
```

**Scalability Patterns**:
- **Model flexibility**: Each agent chooses its own model
- **Error resilience**: Parse failures don't crash system
- **Full observability**: All calls logged
- **Fallback mechanisms**: Graceful degradation on errors

---

## Architecture Overview

### High-Level Flow

```
User → POST /runs → Orchestrator API
    ↓
OrchestratorRunner.execute()
    ↓
    ├─ Iteration 1
    │   ├─ Compile orchestrator context
    │   ├─ Call LLM (orchestrator's model)
    │   ├─ Parse: invoke_agents action
    │   ├─ Governance check: Can invoke intake_agent?
    │   ├─ AgentReActLoopController.execute(intake_agent)
    │   │   ├─ Iteration 1
    │   │   │   ├─ Compile agent context (claim data)
    │   │   │   ├─ Call LLM (agent's model)
    │   │   │   ├─ Parse: use_tools action (schema_validator)
    │   │   │   ├─ Governance check: Can use schema_validator?
    │   │   │   ├─ Invoke tool via Tools Gateway
    │   │   │   └─ Add result to observations
    │   │   ├─ Iteration 2
    │   │   │   ├─ Call LLM (with observations)
    │   │   │   ├─ Parse: final_output action
    │   │   │   └─ Return normalized claim
    │   │   └─ Result: {normalized_claim, validation_status}
    │   ├─ Add intake output to prior_outputs
    │   └─ Add to orchestrator observations
    │
    ├─ Iteration 2
    │   ├─ Call LLM (with updated state)
    │   ├─ Parse: invoke_agents action (coverage_agent)
    │   ├─ AgentReActLoopController.execute(coverage_agent)
    │   │   └─ ... (similar flow, uses policy_snapshot tool)
    │   └─ Add coverage output to prior_outputs
    │
    ├─ ... (fraud, severity, recommendation agents)
    │
    └─ Iteration N
        ├─ Call LLM (all agents executed)
        ├─ Parse: workflow_complete action
        ├─ Validate completion criteria
        └─ Return evidence map

All events logged to JSONL → /storage/sessions/{session_id}.jsonl
Evidence map saved to → /storage/artifacts/{artifact_id}.json
```

---

## Key Scalability Patterns

### 1. **Configuration-Driven Everything**

```
Change behavior without code changes:
- Agent capabilities → agent_registry.json
- Tool access → governance_policies.json
- Model selection → model_profiles.json
- Execution limits → .env variables
```

### 2. **Dynamic Discovery**

```
No hardcoded dependencies:
- Orchestrator discovers agents from registry
- Agents discover tools from registry
- Models selected via profile references
- Governance policies loaded at runtime
```

### 3. **Multi-Provider Support**

```
Agent-level model selection:
- Orchestrator → Claude 3.5 Sonnet (complex reasoning)
- Fraud Agent → GPT-4 (deep analysis)
- Intake Agent → GPT-3.5 Turbo (cost-optimized)
```

### 4. **Bounded Execution**

```
Multi-layer safety:
- Iteration limits per agent (5-10)
- Workflow timeout (300s)
- Token budgets (50K/session)
- Tool invocation limits (50/session)
- LLM call limits (30/session)
```

### 5. **Complete Observability**

```
JSONL event streaming:
- Every agent reasoning logged
- Every tool call logged
- Every LLM call logged (tokens, latency)
- Every policy violation logged
- Full replay capability
```

### 6. **Governance-Driven**

```
Policy enforcement:
- Agent→Agent access control
- Agent→Tool access control
- Duplicate invocation detection
- Session-level limits
- Audit trail generation
```

### 7. **Error Resilience**

```
Graceful degradation:
- LLM call failures → retry with backoff
- Parse failures → fallback responses
- Tool failures → continue with warnings
- Partial outputs on timeout
```

### 8. **Token-Aware Context**

```
Cost optimization:
- Budget allocation (30/50/20)
- Dependency-aware selection
- Recency-biased observations
- Truncation strategies
```

### 9. **Stateless Services**

```
Horizontal scaling:
- Registry manager (singleton but stateless)
- Governance enforcer (session-scoped)
- LLM clients (model-scoped)
- All state in storage/registry
```

### 10. **Hot-Reload Capability**

```
Zero-downtime updates:
- Reload registries without restart
- Update policies at runtime
- Change model profiles dynamically
```

---

## What's Working Now

✅ **Complete ReAct Implementation**: Both orchestrator and worker agents reason and act dynamically

✅ **Multi-Provider LLM**: OpenAI and Claude fully integrated with retry logic

✅ **Dynamic Discovery**: Agents and tools discovered from registries at runtime

✅ **Governance Enforcement**: Policies enforced with audit trails

✅ **Context Management**: Token-aware compilation prevents cost explosion

✅ **Complete Observability**: Every event logged to JSONL for replay

✅ **Error Resilience**: Retry logic, fallbacks, graceful degradation

✅ **Configuration-Driven**: All behavior controlled via registries and .env

---

## Phase 4: Tools Gateway & Mock Tools

**Goal**: Implement production-ready tools service with mock tools and HTTP client integration.

**Status**: ✅ Complete

### 4.1 Tools Gateway FastAPI Service

**File**: `tools/tools_gateway/app/main.py`

**Purpose**: RESTful API service for tool invocation with standardized interface.

**Architecture**:

```python
# FastAPI app with standardized endpoints
app = FastAPI(
    title="Tools Gateway",
    description="Mock tools service for multi-agent insurance processing",
    version="1.0.0"
)

# Tool registry - maps tool_id to execution function
TOOL_REGISTRY = {
    "policy_snapshot": execute_policy_snapshot,
    "fraud_rules": execute_fraud_rules,
    "similarity": execute_similarity,
    "schema_validator": execute_schema_validator,
    "coverage_rules": execute_coverage_rules,
    "decision_rules": execute_decision_rules,
}
```

**Key Endpoints**:

#### 4.1.1 Tool Invocation Endpoint

```python
@app.post("/invoke/{tool_id}", response_model=ToolInvocationResponse)
async def invoke_tool(
    tool_id: str,
    request: ToolInvocationRequest
):
    """
    Invoke a tool by ID.

    Demonstrates: Standardized tool invocation pattern with validation and lineage.
    """
    start_time = datetime.now()

    # Validate tool exists
    if tool_id not in TOOL_REGISTRY:
        raise HTTPException(
            status_code=404,
            detail=f"Tool '{tool_id}' not found"
        )

    try:
        # Get tool execution function
        tool_function = TOOL_REGISTRY[tool_id]

        # Execute tool
        result = tool_function(request.parameters)

        # Calculate execution time
        execution_time_ms = (datetime.now() - start_time).total_seconds() * 1000

        # Build lineage tags
        lineage_tags = {
            "tool_id": tool_id,
            "timestamp": datetime.now().isoformat()
        }
        if request.session_id:
            lineage_tags["session_id"] = request.session_id
        if request.agent_id:
            lineage_tags["agent_id"] = request.agent_id

        return ToolInvocationResponse(
            tool_id=tool_id,
            result=result,
            lineage_tags=lineage_tags,
            timestamp=datetime.now().isoformat(),
            execution_time_ms=execution_time_ms
        )

    except ValueError as e:
        raise HTTPException(status_code=400, detail=f"Validation error: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Execution failed: {str(e)}")
```

**Request/Response Models**:

```python
class ToolInvocationRequest(BaseModel):
    """Request model for tool invocation."""
    parameters: Dict[str, Any] = Field(default_factory=dict)
    session_id: Optional[str] = Field(None)
    agent_id: Optional[str] = Field(None)

class ToolInvocationResponse(BaseModel):
    """Response model for tool invocation."""
    tool_id: str
    result: Dict[str, Any]
    lineage_tags: Dict[str, str]
    timestamp: str
    execution_time_ms: float
```

**Other Endpoints**:
- `GET /` - Health check
- `GET /tools` - List available tools

**Scalability Patterns**:
- **Standardized interface**: All tools follow same contract
- **Lineage tracking**: session_id and agent_id propagated
- **Error handling**: Proper HTTP status codes and error messages
- **Execution metrics**: Track timing for observability
- **Registry pattern**: Easy to add new tools

---

### 4.2 Mock Tool Implementations

All tools implement realistic mock data with production patterns.

#### 4.2.1 Policy Snapshot Tool

**File**: `tools/tools_gateway/app/tools/policy_snapshot.py`

**Purpose**: Retrieve policy coverage information.

**Implementation**:

```python
def execute_policy_snapshot(parameters: Dict[str, Any]) -> Dict[str, Any]:
    """
    Retrieve policy coverage information for a given policy ID.

    Demonstrates: External data retrieval pattern with mock data.
    """
    # Validate required parameters
    if "policy_id" not in parameters:
        raise ValueError("Missing required parameter: policy_id")

    policy_id = parameters["policy_id"]

    # Mock policy database (3 sample policies)
    MOCK_POLICIES = {
        "POL-001": {
            "policy_id": "POL-001",
            "policyholder": "John Doe",
            "policy_type": "Auto Insurance",
            "coverage_limits": {
                "bodily_injury": 100000,
                "property_damage": 50000,
                "collision": 25000,
                "comprehensive": 25000
            },
            "deductibles": {
                "collision": 500,
                "comprehensive": 250
            },
            "status": "active",
            "riders": ["rental_reimbursement", "roadside_assistance"],
            "excluded_perils": ["racing", "commercial_use"]
        },
        # ... POL-002, POL-003
    }

    # Return mock policy or default
    return MOCK_POLICIES.get(policy_id, default_policy)
```

**Output**: Complete policy details with coverage limits, deductibles, riders, exclusions.

---

#### 4.2.2 Fraud Rules Tool

**File**: `tools/tools_gateway/app/tools/fraud_rules.py`

**Purpose**: Detect fraud indicators using rule-based logic.

**Implementation** (7 Rules):

```python
def execute_fraud_rules(parameters: Dict[str, Any]) -> Dict[str, Any]:
    """
    Analyze claim for fraud indicators using rule-based detection.

    Demonstrates: Rule engine pattern with configurable thresholds.
    """
    fraud_indicators = []
    risk_score = 0

    # Rule 1: High claim amount (>$50,000)
    if claim_amount > 50000:
        fraud_indicators.append({
            "rule_id": "FR-001",
            "rule_name": "High Claim Amount",
            "severity": "medium",
            "risk_points": 15
        })
        risk_score += 15

    # Rule 2: Very high claim amount (>$100,000)
    if claim_amount > 100000:
        risk_score += 25

    # Rule 3: New policy claim (within 30 days)
    if days_since_policy_start <= 30:
        risk_score += 30

    # Rule 4: Multiple claims history
    if prior_claims_count >= 3:
        risk_score += 20

    # Rule 5: Suspicious keywords in description
    if len(detected_keywords) >= 2:
        risk_score += 15

    # Rule 6: Inconsistent injury severity and amount
    if injury_severity == "minor" and claim_amount > 25000:
        risk_score += 20

    # Rule 7: Weekend incident
    if incident_day in ["Saturday", "Sunday"]:
        risk_score += 5

    # Determine risk level
    risk_level = "high" if risk_score >= 50 else "medium" if risk_score >= 25 else "low"

    return {
        "fraud_indicators_detected": fraud_indicators,
        "risk_score": risk_score,
        "risk_level": risk_level,
        "requires_siu_review": risk_level == "high",
        "recommendations": recommendations
    }
```

**Rules Implemented**:
1. High claim amount (>$50k)
2. Very high claim amount (>$100k)
3. New policy claim (within 30 days)
4. Multiple claims history (≥3 prior claims)
5. Suspicious keywords in description
6. Inconsistent injury severity vs amount
7. Weekend incident (statistical indicator)

**Output**: Fraud indicators, risk score (0-135), risk level, SIU recommendation.

---

#### 4.2.3 Similarity Tool

**File**: `tools/tools_gateway/app/tools/similarity.py`

**Purpose**: Find similar historical claims.

**Implementation**:

```python
def execute_similarity(parameters: Dict[str, Any]) -> Dict[str, Any]:
    """
    Find similar historical claims based on claim characteristics.

    Demonstrates: Vector similarity search pattern (mocked with rule-based matching).
    """
    # Mock historical claims database (8 claims)
    MOCK_HISTORICAL_CLAIMS = [
        {
            "claim_id": "CLM-2023-0789",
            "loss_type": "collision",
            "claim_amount": 8500,
            "resolution": "approved",
            "payout_amount": 7800,
            "processing_days": 12,
            "fraud_detected": False
        },
        # ... 7 more historical claims
    ]

    # Calculate similarity scores
    similar_claims = []
    for historical_claim in MOCK_HISTORICAL_CLAIMS:
        similarity_score = 0

        # Factor 1: Loss type match (40 points)
        if historical_claim["loss_type"] == loss_type:
            similarity_score += 40

        # Factor 2: Policy type match (20 points)
        if historical_claim["policy_type"] == policy_type:
            similarity_score += 20

        # Factor 3: Location match (15 points)
        if historical_claim["location"] == location:
            similarity_score += 15

        # Factor 4: Claim amount similarity (25 points max)
        amount_diff_pct = abs(historical_claim["claim_amount"] - claim_amount) / claim_amount
        if amount_diff_pct <= 0.2:
            similarity_score += 25

        if similarity_score >= 20:
            similar_claims.append({
                "claim_id": historical_claim["claim_id"],
                "similarity_score": similarity_score,
                "claim_details": historical_claim
            })

    # Return top 5 most similar
    return {
        "similar_claims": sorted(similar_claims, key=lambda x: x["similarity_score"], reverse=True)[:5],
        "aggregate_insights": {
            "average_processing_days": avg_processing_days,
            "fraud_detection_rate": fraud_rate,
            "average_payout_ratio": avg_payout_ratio
        }
    }
```

**Similarity Factors**:
- Loss type match (40 points)
- Policy type match (20 points)
- Location match (15 points)
- Claim amount similarity (25 points)

**Output**: Top 5 similar claims with aggregate insights (avg processing days, fraud rate, payout ratio).

---

#### 4.2.4 Schema Validator Tool

**File**: `tools/tools_gateway/app/tools/schema_validator.py`

**Purpose**: Validate claim data against schema.

**Implementation**:

```python
def execute_schema_validator(parameters: Dict[str, Any]) -> Dict[str, Any]:
    """
    Validate claim data against expected schema.

    Demonstrates: Data validation pattern with detailed error reporting.
    """
    claim_data = parameters.get("claim_data", {})
    validation_errors = []
    validation_warnings = []

    # Required fields validation
    required_fields = ["claim_id", "policy_id", "claim_date", "loss_type", "claim_amount"]
    for field in required_fields:
        if field not in claim_data or claim_data[field] is None:
            validation_errors.append({
                "field": field,
                "error_type": "required_field_missing",
                "severity": "error"
            })

    # Field type validation
    if "claim_amount" in claim_data:
        if not isinstance(claim_data["claim_amount"], (int, float)):
            validation_errors.append({
                "field": "claim_amount",
                "error_type": "invalid_type",
                "severity": "error"
            })

    # Date format validation
    for field in ["claim_date", "incident_date"]:
        if field in claim_data and not is_valid_date(claim_data[field]):
            validation_errors.append({
                "field": field,
                "error_type": "invalid_format",
                "severity": "error"
            })

    # Cross-field validation
    if claim_date < incident_date:
        validation_errors.append({
            "field": "claim_date",
            "error_type": "logical_inconsistency",
            "message": "Claim date cannot be before incident date",
            "severity": "error"
        })

    return {
        "is_valid": len(validation_errors) == 0,
        "validation_errors": validation_errors,
        "validation_warnings": validation_warnings
    }
```

**Validations**:
- Required fields presence
- Field type validation
- Date format validation (ISO 8601)
- Email/phone format validation
- Cross-field logical consistency
- Value range warnings

**Output**: Validation result with detailed errors and warnings.

---

#### 4.2.5 Coverage Rules Tool

**File**: `tools/tools_gateway/app/tools/coverage_rules.py`

**Purpose**: Determine coverage eligibility.

**Implementation** (5 Rules):

```python
def execute_coverage_rules(parameters: Dict[str, Any]) -> Dict[str, Any]:
    """
    Determine coverage eligibility based on policy terms and claim details.

    Demonstrates: Business rules engine pattern for coverage determination.
    """
    # Rule 1: Policy must be active
    if policy_status != "active":
        return denied_response("Policy not active")

    # Rule 2: Check for excluded perils
    for excluded_peril in excluded_perils:
        if excluded_peril in loss_type:
            return denied_response(f"Loss type '{loss_type}' is excluded")

    # Rule 3: Map loss type to coverage limit
    coverage_limit = get_coverage_limit_for_loss_type(loss_type, coverage_limits)
    if coverage_limit == 0:
        return denied_response("No coverage limit for loss type")

    # Rule 4: Get applicable deductible
    deductible_amount = get_deductible_for_loss_type(loss_type, deductibles)

    # Rule 5: Calculate covered amount
    gross_covered = min(claim_amount, coverage_limit)
    net_covered = max(0, gross_covered - deductible_amount)

    if net_covered <= 0:
        determination = "denied"
    elif net_covered < claim_amount:
        determination = "partial"
    else:
        determination = "approved"

    return {
        "coverage_determination": determination,
        "coverage_amount": net_covered,
        "deductible_amount": deductible_amount,
        "reasons": reasons
    }
```

**Rules**:
1. Policy status check (must be active)
2. Peril exclusion check
3. Coverage limit determination
4. Deductible calculation
5. Coverage amount calculation

**Output**: Coverage determination (approved/partial/denied), covered amount, deductible, reasoning.

---

#### 4.2.6 Decision Rules Tool

**File**: `tools/tools_gateway/app/tools/decision_rules.py`

**Purpose**: Recommend action based on analysis results.

**Implementation** (Decision Tree with 7 Branches):

```python
def execute_decision_rules(parameters: Dict[str, Any]) -> Dict[str, Any]:
    """
    Recommend next action for claim based on analysis results.

    Demonstrates: Decision tree pattern for claims processing.
    """
    # Extract analysis results
    coverage_determination = coverage_result.get("coverage_determination")
    fraud_risk_level = fraud_result.get("risk_level")
    severity_level = severity_result.get("complexity_level")

    # Decision Tree Logic

    # Branch 1: Coverage denied
    if coverage_determination == "denied":
        return {
            "recommended_action": "deny_claim",
            "processing_track": "expedited",
            "next_steps": ["Send denial letter", "Close claim"]
        }

    # Branch 2: High fraud risk
    if fraud_risk_level == "high":
        return {
            "recommended_action": "escalate_to_siu",
            "processing_track": "investigation",
            "required_approvals": ["SIU Manager"],
            "next_steps": ["Assign to SIU", "Conduct investigation"]
        }

    # Branch 3: High severity/complexity
    if severity_level in ["high", "critical"]:
        return {
            "recommended_action": "assign_senior_adjuster",
            "processing_track": "complex",
            "required_approvals": ["Senior Claims Manager"]
        }

    # Branch 4: High value claim (>$50,000)
    if coverage_amount > 50000:
        return {
            "recommended_action": "require_manager_approval",
            "processing_track": "standard_with_approval",
            "required_approvals": ["Claims Manager"]
        }

    # Branch 5: Medium complexity + medium fraud
    if severity_level == "medium" and fraud_risk_level == "medium":
        return {
            "recommended_action": "standard_processing_with_review",
            "processing_track": "standard_enhanced"
        }

    # Branch 6: Partial coverage
    if coverage_determination == "partial":
        return {
            "recommended_action": "approve_with_explanation",
            "processing_track": "standard"
        }

    # Branch 7: Standard approval
    return {
        "recommended_action": "approve_and_pay",
        "processing_track": "fast_track",
        "next_steps": ["Validate docs", "Approve payment", "Process within 5 days"]
    }
```

**Decision Branches**:
1. Coverage denied → Expedited denial
2. High fraud risk → SIU investigation
3. High complexity → Senior adjuster
4. High value (>$50k) → Manager approval
5. Medium complexity + fraud → Enhanced review
6. Partial coverage → Approval with explanation
7. Standard → Fast track approval

**Output**: Recommended action, processing track, required approvals, next steps, timeline estimate.

---

### 4.3 Tools Gateway Client

**File**: `backend/orchestrator/app/services/tools_gateway_client.py`

**Purpose**: HTTP client for invoking tools with retry logic and error handling.

**Architecture**:

```python
class ToolsGatewayClient:
    """
    HTTP client for invoking tools through Tools Gateway service.

    Demonstrates: Production-grade service client with retry and error handling.
    """

    def __init__(
        self,
        base_url: Optional[str] = None,
        session_id: Optional[str] = None,
        agent_id: Optional[str] = None
    ):
        self.base_url = base_url or "http://tools_gateway:8001"
        self.session_id = session_id
        self.agent_id = agent_id
        self.max_retries = 3
        self.timeout_seconds = 30
        self.client = httpx.Client(timeout=self.timeout_seconds)
```

**Key Methods**:

#### 4.3.1 Tool Invocation with Retry

```python
def invoke_tool(
    self,
    tool_id: str,
    parameters: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Invoke a tool by ID.

    Demonstrates: Retry logic with exponential backoff and timeout handling.
    """
    url = f"{self.base_url}/invoke/{tool_id}"

    # Build request payload
    payload = {
        "parameters": parameters,
        "session_id": self.session_id,
        "agent_id": self.agent_id
    }

    # Retry loop with exponential backoff
    for attempt in range(1, self.max_retries + 1):
        try:
            response = self.client.post(url, json=payload)

            if response.status_code == 200:
                return response.json()

            elif response.status_code == 404:
                # Tool not found - don't retry
                raise ToolsGatewayError(f"Tool '{tool_id}' not found")

            elif response.status_code == 400:
                # Validation error - don't retry
                raise ToolsGatewayError(f"Validation error: {response.json()['detail']}")

            else:
                # Server error - retry
                last_error = ToolsGatewayError(f"HTTP {response.status_code}")

        except httpx.TimeoutException as e:
            last_error = ToolsGatewayError(f"Timeout after {self.timeout_seconds}s")

        except httpx.RequestError as e:
            last_error = ToolsGatewayError(f"Connection error: {str(e)}")

        # Retry with exponential backoff
        if attempt < self.max_retries:
            delay = 1.0 * (2.0 ** (attempt - 1))
            time.sleep(delay)

    raise last_error
```

**Error Handling**:
- **404 (Not Found)**: Don't retry, raise immediately
- **400 (Validation)**: Don't retry, raise immediately
- **500 (Server Error)**: Retry with backoff
- **Timeout**: Retry with backoff
- **Connection Error**: Retry with backoff

#### 4.3.2 Batch Invocation

```python
def invoke_tools_batch(self, tool_requests: list) -> list:
    """
    Invoke multiple tools sequentially.

    Returns list of results (same order as requests).
    """
    results = []
    for tool_req in tool_requests:
        try:
            result = self.invoke_tool(tool_req["tool_id"], tool_req["parameters"])
            results.append({"status": "success", "result": result})
        except ToolsGatewayError as e:
            results.append({"status": "error", "error": str(e)})
    return results
```

#### 4.3.3 Health Check

```python
def health_check(self) -> bool:
    """Check if Tools Gateway is healthy."""
    try:
        response = self.client.get(f"{self.base_url}/", timeout=5.0)
        return response.status_code == 200
    except:
        return False
```

**Scalability Patterns**:
- **Retry with exponential backoff**: Resilience against transient failures
- **Timeout protection**: Prevents hanging calls
- **Error classification**: Different handling for different error types
- **Lineage propagation**: session_id and agent_id tracked
- **Context manager support**: Proper resource cleanup
- **Health check**: Circuit breaker enablement

**Factory Function**:

```python
def create_tools_gateway_client(
    session_id: Optional[str] = None,
    agent_id: Optional[str] = None
) -> ToolsGatewayClient:
    """Factory function to create ToolsGatewayClient."""
    return ToolsGatewayClient(session_id=session_id, agent_id=agent_id)
```

---

### 4.4 Docker Configuration

**File**: `tools/tools_gateway/Dockerfile`

```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY app /app/app
EXPOSE 8001
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8001", "--reload"]
```

**File**: `tools/tools_gateway/requirements.txt`

```txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
python-dateutil==2.8.2
orjson==3.9.10
```

---

### 4.5 Key Scalability Patterns Demonstrated

1. ✅ **Service-to-Service Communication**: HTTP-based tool invocation
2. ✅ **Standardized Interface**: All tools follow same contract
3. ✅ **Retry with Exponential Backoff**: Resilience against transient failures
4. ✅ **Timeout Protection**: Prevents hanging operations
5. ✅ **Lineage Tracking**: session_id and agent_id propagation
6. ✅ **Error Classification**: Different handling for different error types
7. ✅ **Mock Data Realism**: Production-like data for testing
8. ✅ **Registry Pattern**: Easy tool addition without code changes
9. ✅ **Execution Metrics**: Timing and observability
10. ✅ **Containerization**: Ready for deployment

---

### 4.6 Phase 4 Summary

**Files Created**:
```
tools/tools_gateway/
├── Dockerfile
├── requirements.txt
└── app/
    ├── __init__.py
    ├── main.py (FastAPI service, 3 endpoints, 250+ lines)
    └── tools/
        ├── __init__.py
        ├── policy_snapshot.py (130 lines)
        ├── fraud_rules.py (180 lines)
        ├── similarity.py (210 lines)
        ├── schema_validator.py (240 lines)
        ├── coverage_rules.py (280 lines)
        └── decision_rules.py (270 lines)

backend/orchestrator/app/services/
└── tools_gateway_client.py (270 lines)
```

**Total Lines Added**: ~1,830 lines of production-ready code

**Completion Criteria Met**:
- ✅ Tools Gateway FastAPI service fully functional
- ✅ All 6 mock tools implemented with realistic data
- ✅ ToolsGatewayClient integrated into orchestrator
- ✅ Retry logic and error handling implemented
- ✅ Lineage tracking for observability
- ✅ Containerized and ready for docker-compose

---

## Phase 5: Agent Output Schemas ✅ COMPLETE

**Purpose**: Implement production-grade schema validation for all agent outputs using Pydantic v2, enabling type safety, runtime validation, and detailed error reporting.

**Status**: ✅ All tasks completed (6/6)

**Completion Date**: Phase 5 implementation complete

---

### 5.1 Schema Configuration

Added configurable schema validation settings to support flexible validation behavior across environments.

**File**: `backend/orchestrator/app/config.py`

**Changes Made**:

```python
class SchemaSettings(BaseModel):
    """Schema validation settings for agent outputs.

    Demonstrates: Configuration-driven validation behavior
    """
    default_version: str = Field(
        default="1.0",
        description="Default schema version"
    )
    strict_validation: bool = Field(
        default=True,
        description="Enforce strict schema validation"
    )
    validation_failure_limit: int = Field(
        default=3,
        description="Max validation failures before agent errors"
    )
    log_validation_sample: bool = Field(
        default=True,
        description="Log output sample on validation failure"
    )
    max_validation_sample_chars: int = Field(
        default=500,
        description="Max chars to log in validation samples"
    )


class Config(BaseModel):
    """Complete system configuration."""
    # ... existing fields ...
    schema: SchemaSettings = Field(default_factory=SchemaSettings)


def load_config() -> Config:
    """Load configuration from environment variables."""
    return Config(
        # ... existing config loading ...

        # Schema validation settings
        schema=SchemaSettings(
            default_version=os.getenv("SCHEMA_DEFAULT_VERSION", "1.0"),
            strict_validation=os.getenv("SCHEMA_STRICT_VALIDATION", "true").lower() == "true",
            validation_failure_limit=int(os.getenv("SCHEMA_VALIDATION_FAILURE_LIMIT", "3")),
            log_validation_sample=os.getenv("SCHEMA_LOG_VALIDATION_SAMPLE", "true").lower() == "true",
            max_validation_sample_chars=int(os.getenv("SCHEMA_MAX_VALIDATION_SAMPLE_CHARS", "500"))
        )
    )
```

**Demonstrates**:
- Configurable validation behavior
- Environment-based configuration
- Production-ready defaults with override capability

**Design Decision**: Made validation failure limits and logging behavior configurable to support different deployment scenarios (dev vs prod) without code changes.

---

### 5.2 Schema Package Structure

Created centralized schema package with clear exports for all validation-related functionality.

**File**: `backend/orchestrator/app/schemas/__init__.py` (Created - 89 lines)

```python
"""
Agent Output Schemas - Pydantic models for structured output validation.

Provides type-safe, validated schemas for all agent outputs enabling:
- Runtime validation
- Type safety
- Documentation
- Observability
- Context engineering

Demonstrates scalability patterns:
- Schema-driven validation
- Configurable validation behavior
- Detailed error reporting
- Version tracking
"""

from .agent_outputs import (
    AgentOutputBase,
    DataQualityIssue,
    IntakeAgentOutput,
    CoverageDeterminationType,
    CoverageAgentOutput,
    RiskBand,
    FraudIndicator,
    FraudAgentOutput,
    ComplexityLevel,
    SeverityAgentOutput,
    ProcessingTrack,
    RecommendationAgentOutput,
    Evidence,
    Decision,
    ExplainabilityAgentOutput,
    CompletionReason,
    OrchestratorOutput
)

from .validators import (
    validate_agent_output,
    get_schema_for_agent,
    get_schema_json,
    list_available_schemas,
    SchemaValidationError,
    AGENT_OUTPUT_SCHEMAS
)

__all__ = [
    # Base models
    "AgentOutputBase",

    # Intake agent
    "DataQualityIssue",
    "IntakeAgentOutput",

    # Coverage agent
    "CoverageDeterminationType",
    "CoverageAgentOutput",

    # Fraud agent
    "RiskBand",
    "FraudIndicator",
    "FraudAgentOutput",

    # Severity agent
    "ComplexityLevel",
    "SeverityAgentOutput",

    # Recommendation agent
    "ProcessingTrack",
    "RecommendationAgentOutput",

    # Explainability agent
    "Evidence",
    "Decision",
    "ExplainabilityAgentOutput",

    # Orchestrator
    "CompletionReason",
    "OrchestratorOutput",

    # Validators
    "validate_agent_output",
    "get_schema_for_agent",
    "get_schema_json",
    "list_available_schemas",
    "SchemaValidationError",
    "AGENT_OUTPUT_SCHEMAS"
]
```

**Demonstrates**:
- Clean package interface
- Type-safe exports
- Documentation-driven design

---

### 5.3 Agent Output Schemas

Implemented Pydantic v2 models for all 7 agent types with full validation.

**File**: `backend/orchestrator/app/schemas/agent_outputs.py` (Created - 410 lines)

#### 5.3.1 Base Schema

```python
from pydantic import BaseModel, Field, field_validator
from typing import List, Dict, Any, Optional
from datetime import datetime
from enum import Enum


class AgentOutputBase(BaseModel):
    """Base class for all agent outputs.

    Demonstrates: Consistent metadata across all agent outputs
    """
    agent_id: str = Field(..., description="Agent that produced this output")
    timestamp: str = Field(..., description="ISO 8601 timestamp")
    version: str = Field(default="1.0", description="Schema version")

    class Config:
        extra = "forbid"  # Strict mode - no extra fields allowed
```

**Design Decision**: Used `extra = "forbid"` to enforce strict validation and catch schema drift early.

#### 5.3.2 Intake Agent Schema

```python
class DataQualityIssue(BaseModel):
    """Data quality issue detected during intake."""
    field: str = Field(..., description="Field with issue")
    issue_type: str = Field(..., description="Type of issue (missing, invalid, inconsistent)")
    severity: str = Field(..., description="Severity: low, medium, high")
    message: str = Field(..., description="Human-readable description")


class IntakeAgentOutput(AgentOutputBase):
    """Output schema for intake_agent.

    Demonstrates: Data quality scoring and normalization tracking
    """

    normalized_claim: Dict[str, Any] = Field(
        ...,
        description="Normalized and validated claim data"
    )

    data_quality_score: float = Field(
        ...,
        ge=0.0,
        le=1.0,
        description="Overall data quality score (0-1)"
    )

    data_quality_issues: List[DataQualityIssue] = Field(
        default_factory=list,
        description="List of data quality issues found"
    )

    validation_passed: bool = Field(
        ...,
        description="Whether claim passed basic validation"
    )

    normalization_changes: List[str] = Field(
        default_factory=list,
        description="List of normalization changes applied"
    )

    @field_validator('data_quality_score')
    @classmethod
    def score_range_check(cls, v):
        if not 0 <= v <= 1:
            raise ValueError('data_quality_score must be between 0 and 1')
        return v
```

**Demonstrates**:
- Custom validators for business rules
- Nested models for complex structures
- Default factories for mutable fields

#### 5.3.3 Coverage Agent Schema

```python
class CoverageDeterminationType(str, Enum):
    """Coverage determination outcomes."""
    APPROVED = "approved"
    PARTIAL = "partial"
    DENIED = "denied"
    PENDING_REVIEW = "pending_review"


class CoverageAgentOutput(AgentOutputBase):
    """Output schema for coverage_agent.

    Demonstrates: Financial calculation validation with constraints
    """

    coverage_determination: CoverageDeterminationType = Field(
        ...,
        description="Coverage determination result"
    )

    coverage_amount: float = Field(
        ...,
        ge=0.0,
        description="Amount covered by policy (after deductibles)"
    )

    deductible_amount: float = Field(
        ...,
        ge=0.0,
        description="Applicable deductible amount"
    )

    coverage_percentage: float = Field(
        ...,
        ge=0.0,
        le=100.0,
        description="Percentage of claim covered"
    )

    exclusions_triggered: List[str] = Field(
        default_factory=list,
        description="List of policy exclusions that apply"
    )

    coverage_limits_applied: Dict[str, float] = Field(
        default_factory=dict,
        description="Coverage limits that were applied"
    )

    reasoning: str = Field(
        ...,
        description="Detailed explanation of coverage determination"
    )
```

**Demonstrates**:
- Enum-based constrained choices
- Numeric constraints (ge, le)
- Explainability through required reasoning field

#### 5.3.4 Fraud Agent Schema

```python
class RiskBand(str, Enum):
    """Fraud risk classification."""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class FraudIndicator(BaseModel):
    """Individual fraud indicator."""
    indicator_id: str = Field(..., description="Unique indicator ID")
    indicator_name: str = Field(..., description="Human-readable name")
    severity: str = Field(..., description="Severity level")
    confidence: float = Field(..., ge=0.0, le=1.0, description="Confidence score")
    description: str = Field(..., description="Why this indicator was triggered")


class FraudAgentOutput(AgentOutputBase):
    """Output schema for fraud_agent.

    Demonstrates: Risk assessment with evidence tracking
    """

    fraud_score: float = Field(
        ...,
        ge=0.0,
        le=1.0,
        description="Overall fraud risk score (0-1)"
    )

    risk_band: RiskBand = Field(
        ...,
        description="Risk classification"
    )

    triggered_indicators: List[FraudIndicator] = Field(
        default_factory=list,
        description="All fraud indicators that were triggered"
    )

    siu_referral_required: bool = Field(
        ...,
        description="Whether SIU referral is required"
    )

    similar_claims_analysis: Optional[Dict[str, Any]] = Field(
        None,
        description="Analysis of similar historical claims"
    )

    rationale: str = Field(
        ...,
        description="Detailed fraud assessment rationale"
    )
```

**Demonstrates**:
- Evidence-based decision tracking
- Optional fields for conditional data
- Structured indicator representation

#### 5.3.5 Severity Agent Schema

```python
class ComplexityLevel(str, Enum):
    """Claim complexity classification."""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class SeverityAgentOutput(AgentOutputBase):
    """Output schema for severity_agent.

    Demonstrates: Complexity assessment with resource estimation
    """

    complexity_level: ComplexityLevel = Field(
        ...,
        description="Overall complexity classification"
    )

    complexity_score: float = Field(
        ...,
        ge=0.0,
        le=1.0,
        description="Numerical complexity score (0-1)"
    )

    complexity_factors: List[str] = Field(
        default_factory=list,
        description="Factors contributing to complexity"
    )

    estimated_processing_days: int = Field(
        ...,
        ge=1,
        description="Estimated processing time in days"
    )

    required_expertise_level: str = Field(
        ...,
        description="Required adjuster expertise level"
    )

    special_handling_required: bool = Field(
        ...,
        description="Whether special handling is needed"
    )

    assessment_rationale: str = Field(
        ...,
        description="Detailed complexity assessment"
    )
```

**Demonstrates**:
- Resource planning through validated estimates
- Factor tracking for transparency
- Expertise matching support

#### 5.3.6 Recommendation Agent Schema

```python
class ProcessingTrack(str, Enum):
    """Processing track classification."""
    FAST_TRACK = "fast_track"
    STANDARD = "standard"
    STANDARD_ENHANCED = "standard_enhanced"
    COMPLEX = "complex"
    INVESTIGATION = "investigation"
    EXPEDITED = "expedited"


class RecommendationAgentOutput(AgentOutputBase):
    """Output schema for recommendation_agent.

    Demonstrates: Action planning with confidence scoring
    """

    recommended_action: str = Field(
        ...,
        description="Primary recommended action"
    )

    action_priority: str = Field(
        ...,
        description="Priority level: low, medium, high, urgent"
    )

    processing_track: ProcessingTrack = Field(
        ...,
        description="Recommended processing track"
    )

    required_approvals: List[str] = Field(
        default_factory=list,
        description="List of required approvals"
    )

    next_steps: List[str] = Field(
        ...,
        min_length=1,
        description="Ordered list of next steps"
    )

    estimated_timeline_days: int = Field(
        ...,
        ge=1,
        description="Estimated timeline for resolution"
    )

    confidence: float = Field(
        ...,
        ge=0.0,
        le=1.0,
        description="Confidence in recommendation"
    )

    rationale: str = Field(
        ...,
        description="Detailed recommendation rationale"
    )
```

**Demonstrates**:
- Workflow routing decisions
- Actionable next steps with validation
- Approval chain tracking

#### 5.3.7 Explainability Agent Schema

```python
class Evidence(BaseModel):
    """Individual piece of evidence."""
    source: str = Field(..., description="Source agent or data source")
    evidence_type: str = Field(..., description="Type of evidence")
    summary: str = Field(..., description="Evidence summary")
    weight: float = Field(..., ge=0.0, le=1.0, description="Weight/importance")


class Decision(BaseModel):
    """Final decision structure."""
    outcome: str = Field(..., description="Decision outcome")
    confidence: float = Field(..., ge=0.0, le=1.0, description="Decision confidence")
    basis: str = Field(..., description="Primary basis for decision")


class ExplainabilityAgentOutput(AgentOutputBase):
    """Output schema for explainability_agent (Evidence Map).

    Demonstrates: Comprehensive decision explanation with evidence chain
    """

    decision: Decision = Field(
        ...,
        description="Final decision with confidence"
    )

    supporting_evidence: List[Evidence] = Field(
        ...,
        min_length=1,
        description="All supporting evidence"
    )

    assumptions: List[str] = Field(
        default_factory=list,
        description="Key assumptions made"
    )

    limitations: List[str] = Field(
        default_factory=list,
        description="Limitations of the analysis"
    )

    agent_chain: List[str] = Field(
        ...,
        min_length=1,
        description="Ordered list of agents executed"
    )

    alternative_outcomes: Optional[List[Dict[str, Any]]] = Field(
        None,
        description="Alternative outcomes considered"
    )

    explanation: str = Field(
        ...,
        description="Human-readable explanation of entire analysis"
    )
```

**Demonstrates**:
- Evidence-based explainability
- Decision transparency
- Assumption and limitation tracking
- Agent chain visibility

#### 5.3.8 Orchestrator Output Schema

```python
class CompletionReason(str, Enum):
    """Workflow completion reasons."""
    ALL_OBJECTIVES_ACHIEVED = "all_objectives_achieved"
    MAX_ITERATIONS_REACHED = "max_iterations_reached"
    TIMEOUT = "timeout"
    TOKEN_BUDGET_EXCEEDED = "token_budget_exceeded"
    ERROR = "error"


class OrchestratorOutput(AgentOutputBase):
    """Output schema for orchestrator_agent.

    Demonstrates: Workflow completion tracking with metrics
    """

    status: str = Field(
        ...,
        description="Completion status: completed, completed_with_warning, failed"
    )

    completion_reason: CompletionReason = Field(
        ...,
        description="Reason for completion"
    )

    evidence_map: Dict[str, Any] = Field(
        ...,
        description="Final evidence map (can be complete ExplainabilityAgentOutput or partial auto-generated)"
    )

    agents_executed: List[str] = Field(
        ...,
        min_length=1,
        description="List of agents executed in order"
    )

    total_iterations: int = Field(
        ...,
        ge=1,
        description="Total orchestrator iterations"
    )

    total_agent_invocations: int = Field(
        ...,
        ge=1,
        description="Total number of agent invocations"
    )

    warnings: List[str] = Field(
        default_factory=list,
        description="Any warnings encountered"
    )

    errors: List[str] = Field(
        default_factory=list,
        description="Any errors encountered"
    )
```

**Design Decision**: Used `Dict[str, Any]` for `evidence_map` instead of strict `ExplainabilityAgentOutput` type to handle both complete evidence maps (from explainability_agent) and partial auto-generated maps (from Tier 3 forced completions).

**Demonstrates**:
- Flexible schema design for varying completion scenarios
- Comprehensive execution metrics
- Error and warning tracking

---

### 5.4 Schema Validation Utilities

Implemented production-grade validation utilities with detailed error reporting and observability.

**File**: `backend/orchestrator/app/schemas/validators.py` (Created - 287 lines)

#### 5.4.1 Schema Registry

```python
from typing import Dict, Any, Type, Optional
from pydantic import ValidationError
from datetime import datetime
from .agent_outputs import (
    AgentOutputBase,
    IntakeAgentOutput,
    CoverageAgentOutput,
    FraudAgentOutput,
    SeverityAgentOutput,
    RecommendationAgentOutput,
    ExplainabilityAgentOutput,
    OrchestratorOutput
)


# Schema registry - maps agent_id to output schema class
AGENT_OUTPUT_SCHEMAS: Dict[str, Type[AgentOutputBase]] = {
    "intake_agent": IntakeAgentOutput,
    "coverage_agent": CoverageAgentOutput,
    "fraud_agent": FraudAgentOutput,
    "severity_agent": SeverityAgentOutput,
    "recommendation_agent": RecommendationAgentOutput,
    "explainability_agent": ExplainabilityAgentOutput,
    "orchestrator_agent": OrchestratorOutput
}
```

**Demonstrates**: Registry pattern for dynamic schema discovery

#### 5.4.2 Custom Validation Exception

```python
class SchemaValidationError(Exception):
    """
    Exception raised when schema validation fails.

    Includes detailed error information for observability and debugging.
    """

    def __init__(self, message: str, errors: Optional[list] = None):
        super().__init__(message)
        self.errors = errors or []
        self.timestamp = datetime.utcnow().isoformat() + "Z"
```

**Demonstrates**: Context-aware exceptions with timestamp tracking

#### 5.4.3 Core Validation Function

```python
def validate_agent_output(
    agent_id: str,
    output_data: Dict[str, Any],
    version: Optional[str] = None
) -> AgentOutputBase:
    """
    Validate agent output against its schema.

    Demonstrates: Production-grade validation with detailed error reporting.

    Args:
        agent_id: Agent ID to look up schema
        output_data: Raw output data from agent
        version: Optional schema version override

    Returns:
        Validated output model

    Raises:
        SchemaValidationError: If validation fails with detailed error list

    Example:
        >>> output = validate_agent_output("fraud_agent", {
        ...     "fraud_score": 0.75,
        ...     "risk_band": "high",
        ...     "siu_referral_required": True,
        ...     "rationale": "Multiple fraud indicators detected"
        ... })
    """
    # Get schema for agent
    schema_class = AGENT_OUTPUT_SCHEMAS.get(agent_id)

    if not schema_class:
        raise SchemaValidationError(
            f"No output schema defined for agent '{agent_id}'. "
            f"Available agents: {', '.join(AGENT_OUTPUT_SCHEMAS.keys())}"
        )

    try:
        # Ensure required metadata fields are present
        if "agent_id" not in output_data:
            output_data["agent_id"] = agent_id

        if "timestamp" not in output_data:
            output_data["timestamp"] = datetime.utcnow().isoformat() + "Z"

        # Apply version override if provided
        if version and "version" not in output_data:
            output_data["version"] = version

        # Validate and parse
        validated_output = schema_class(**output_data)

        return validated_output

    except ValidationError as e:
        # Format validation errors for observability
        formatted_errors = []
        for error in e.errors():
            field_path = " -> ".join(str(loc) for loc in error["loc"])
            error_msg = error["msg"]
            error_type = error["type"]

            formatted_errors.append({
                "field": field_path,
                "message": error_msg,
                "type": error_type,
                "input": error.get("input")
            })

        # Create detailed error message
        error_summary = "\n".join(
            f"  • {err['field']}: {err['message']}"
            for err in formatted_errors
        )

        raise SchemaValidationError(
            f"Output validation failed for agent '{agent_id}':\n{error_summary}",
            errors=formatted_errors
        )

    except Exception as e:
        # Catch any other validation errors
        raise SchemaValidationError(
            f"Unexpected validation error for agent '{agent_id}': {str(e)}"
        )
```

**Demonstrates**:
- Auto-injection of metadata fields
- Detailed error formatting
- Observability-friendly error messages

#### 5.4.4 Schema Discovery Utilities

```python
def get_schema_for_agent(agent_id: str) -> Optional[Type[AgentOutputBase]]:
    """
    Get output schema class for agent.

    Args:
        agent_id: Agent ID

    Returns:
        Schema class or None if not found

    Example:
        >>> schema = get_schema_for_agent("fraud_agent")
        >>> print(schema.__name__)
        'FraudAgentOutput'
    """
    return AGENT_OUTPUT_SCHEMAS.get(agent_id)


def get_schema_json(agent_id: str) -> Optional[Dict[str, Any]]:
    """
    Get JSON schema for agent output.

    Useful for API documentation and client code generation.

    Args:
        agent_id: Agent ID

    Returns:
        JSON schema dict or None if agent not found

    Example:
        >>> schema_json = get_schema_json("fraud_agent")
        >>> print(schema_json["properties"]["fraud_score"])
        {'type': 'number', 'minimum': 0.0, 'maximum': 1.0, ...}
    """
    schema_class = AGENT_OUTPUT_SCHEMAS.get(agent_id)
    if schema_class:
        return schema_class.model_json_schema()
    return None


def list_available_schemas() -> Dict[str, str]:
    """
    List all available agent output schemas.

    Returns:
        Dict mapping agent_id to schema class name

    Example:
        >>> schemas = list_available_schemas()
        >>> for agent_id, schema_name in schemas.items():
        ...     print(f"{agent_id}: {schema_name}")
    """
    return {
        agent_id: schema_class.__name__
        for agent_id, schema_class in AGENT_OUTPUT_SCHEMAS.items()
    }
```

**Demonstrates**: Self-documenting API with usage examples

#### 5.4.5 Partial Validation Support

```python
def validate_partial_output(
    agent_id: str,
    output_data: Dict[str, Any],
    required_fields: Optional[list] = None
) -> tuple[bool, list]:
    """
    Validate partial output (for incomplete agent executions).

    Less strict than full validation - only checks specified required fields.

    Args:
        agent_id: Agent ID
        output_data: Partial output data
        required_fields: List of field names that must be present

    Returns:
        Tuple of (is_valid, list_of_errors)

    Example:
        >>> valid, errors = validate_partial_output(
        ...     "fraud_agent",
        ...     {"fraud_score": 0.5},
        ...     required_fields=["fraud_score", "risk_band"]
        ... )
        >>> print(valid, errors)
        False ['Missing required field: risk_band']
    """
    errors = []

    # Check if schema exists
    if agent_id not in AGENT_OUTPUT_SCHEMAS:
        errors.append(f"Unknown agent_id: {agent_id}")
        return False, errors

    # Check required fields if specified
    if required_fields:
        for field in required_fields:
            if field not in output_data:
                errors.append(f"Missing required field: {field}")

    # Check if output_data is a dict
    if not isinstance(output_data, dict):
        errors.append(f"Output must be a dict, got {type(output_data).__name__}")

    return len(errors) == 0, errors
```

**Demonstrates**: Graceful handling of incomplete outputs (useful for error recovery)

---

### 5.5 Integration with Agent ReAct Loop

Enhanced the AgentReActLoopController with production-grade schema validation.

**File**: `backend/orchestrator/app/services/agent_react_loop.py` (Modified)

**Changes Made**:

```python
import logging
from ..config import get_config

logger = logging.getLogger(__name__)

class AgentReActLoopController:
    def __init__(self, ...):
        # ... existing initialization ...
        self._validation_failures = 0  # Track validation failures for safety threshold

    def _validate_output(self, output: Optional[Dict[str, Any]]) -> bool:
        """
        Validate agent output against schema.

        Demonstrates:
        - Production-grade validation with Pydantic schemas
        - Configurable failure limits
        - Comprehensive observability (dual logging)
        - Context engineering (session/agent/iteration tracking)

        Args:
            output: Raw agent output to validate

        Returns:
            bool: True if valid, False if invalid
        """
        from ..schemas.validators import validate_agent_output, SchemaValidationError

        if not output:
            logger.warning(
                f"Output validation failed for {self.agent_id}: output is None or empty "
                f"[session={self.session_id}, iter={self.iteration}]"
            )
            return False

        config = get_config()

        try:
            # Validate using Pydantic schema
            validated = validate_agent_output(self.agent_id, output)

            # Log successful validation event (structured)
            self._log_event("output_validated", {
                "agent_id": self.agent_id,
                "iteration": self.iteration,
                "schema_version": validated.version,
                "validation_attempt": self._validation_failures + 1
            })

            # Reset failure counter on success
            self._validation_failures = 0

            logger.info(
                f"Output validation succeeded for {self.agent_id} "
                f"[session={self.session_id}, iter={self.iteration}, version={validated.version}]"
            )
            return True

        except SchemaValidationError as e:
            # Increment failure counter
            self._validation_failures += 1
            max_failures = config.schema.validation_failure_limit

            # Prepare output sample for logging (if enabled)
            output_sample = None
            if config.schema.log_validation_sample:
                output_str = str(output)
                max_chars = config.schema.max_validation_sample_chars
                output_sample = output_str[:max_chars] + ("..." if len(output_str) > max_chars else "")

            # Log validation failure event (structured for observability)
            self._log_event("output_validation_failed", {
                "agent_id": self.agent_id,
                "iteration": self.iteration,
                "validation_attempt": self._validation_failures,
                "max_attempts": max_failures,
                "error_message": str(e),
                "error_details": e.errors if hasattr(e, 'errors') else [],
                "will_retry": self._validation_failures < max_failures,
                "output_sample": output_sample
            })

            # Python log for real-time monitoring
            logger.warning(
                f"Output validation failed for {self.agent_id} "
                f"[session={self.session_id}, iter={self.iteration}, "
                f"attempt={self._validation_failures}/{max_failures}]: {str(e)}"
            )

            # Check if limit exceeded
            if self._validation_failures >= max_failures:
                logger.error(
                    f"Max validation failures ({max_failures}) exceeded for {self.agent_id}. "
                    f"[session={self.session_id}] Agent execution should terminate."
                )

                self._log_event("validation_failure_limit_exceeded", {
                    "agent_id": self.agent_id,
                    "iteration": self.iteration,
                    "total_failures": self._validation_failures,
                    "limit": max_failures
                })

            return False
```

**Demonstrates**:
- **Dual-Layer Logging**:
  - Layer 1: Structured JSONL events via `_log_event()` for replay/observability
  - Layer 2: Python logging via `logger` for real-time monitoring
- **Context Engineering**: Every log includes session_id, agent_id, iteration
- **Configurable Safety**: Uses `validation_failure_limit` from config
- **Graceful Degradation**: Returns False (not exception) to allow ReAct retry
- **Detailed Observability**: Logs validation attempt count, error details, output samples

**Design Decision**: Chose to return `False` instead of raising an exception to allow the ReAct loop to retry with corrected output. After exceeding the failure limit, the loop will naturally terminate.

---

### 5.6 Comprehensive Test Suite

Created comprehensive test script with 8 test cases covering all schemas and validation utilities.

**File**: `backend/orchestrator/test_schemas.py` (Created - 375 lines)

```python
#!/usr/bin/env python3
"""
Test script for Phase 5: Agent Output Schema Validation

Tests all agent output schemas with sample data to verify:
- Schema validation works correctly
- Required fields are enforced
- Type constraints are validated
- Error messages are clear and actionable
"""

import sys
from datetime import datetime
from pathlib import Path

# Add app to path
sys.path.insert(0, str(Path(__file__).parent))

from app.schemas import (
    validate_agent_output,
    get_schema_for_agent,
    list_available_schemas,
    SchemaValidationError,
    IntakeAgentOutput,
    CoverageAgentOutput,
    FraudAgentOutput,
    SeverityAgentOutput,
    RecommendationAgentOutput,
    ExplainabilityAgentOutput
)


def test_intake_agent():
    """Test IntakeAgentOutput schema."""
    print_section("Testing Intake Agent Schema")

    # Valid output
    valid_data = {
        "agent_id": "intake_agent",
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "normalized_claim": {
            "claim_id": "CLM-001",
            "policy_id": "POL-001",
            "claim_amount": 15000.0
        },
        "data_quality_score": 0.95,
        "validation_passed": True,
        "data_quality_issues": [
            {
                "field": "incident_date",
                "issue_type": "inconsistent",
                "severity": "low",
                "message": "Date format standardized"
            }
        ],
        "normalization_changes": ["standardized_date_format", "uppercase_policy_id"]
    }

    try:
        validated = validate_agent_output("intake_agent", valid_data)
        print(f"✓ Valid intake output validated successfully")
        print(f"  - Agent ID: {validated.agent_id}")
        print(f"  - Quality Score: {validated.data_quality_score}")
        print(f"  - Validation Passed: {validated.validation_passed}")
        print(f"  - Issues: {len(validated.data_quality_issues)}")
    except SchemaValidationError as e:
        print(f"✗ FAILED: {e}")
        return False

    # Invalid output - score out of range
    invalid_data = valid_data.copy()
    invalid_data["data_quality_score"] = 1.5  # Invalid: > 1.0

    try:
        validated = validate_agent_output("intake_agent", invalid_data)
        print(f"✗ FAILED: Should have rejected score > 1.0")
        return False
    except SchemaValidationError as e:
        print(f"✓ Correctly rejected invalid score: {e}")

    return True


def test_missing_required_fields():
    """Test that missing required fields are caught."""
    print_section("Testing Missing Required Fields")

    incomplete_data = {
        "agent_id": "fraud_agent",
        "fraud_score": 0.5
        # Missing: risk_band, siu_referral_required, rationale
    }

    try:
        validated = validate_agent_output("fraud_agent", incomplete_data)
        print(f"✗ FAILED: Should have rejected incomplete data")
        return False
    except SchemaValidationError as e:
        print(f"✓ Correctly rejected incomplete data")
        print(f"  Error details: {e}")
        return True


def test_list_schemas():
    """Test schema listing utility."""
    print_section("Testing Schema Discovery")

    schemas = list_available_schemas()
    print(f"Available schemas: {len(schemas)}")
    for agent_id, schema_name in schemas.items():
        print(f"  - {agent_id}: {schema_name}")

        # Get schema class
        schema_class = get_schema_for_agent(agent_id)
        if schema_class:
            print(f"    ✓ Schema class retrieved: {schema_class.__name__}")
        else:
            print(f"    ✗ Failed to retrieve schema class")

    return len(schemas) == 7  # Should have 7 agent schemas


def main():
    """Run all schema validation tests."""
    print_section("Phase 5: Agent Output Schema Validation Tests")
    print("Testing all agent output schemas with sample data...\n")

    tests = [
        ("Intake Agent", test_intake_agent),
        ("Coverage Agent", test_coverage_agent),
        ("Fraud Agent", test_fraud_agent),
        ("Severity Agent", test_severity_agent),
        ("Recommendation Agent", test_recommendation_agent),
        ("Explainability Agent", test_explainability_agent),
        ("Missing Fields", test_missing_required_fields),
        ("Schema Discovery", test_list_schemas)
    ]

    results = []
    for name, test_func in tests:
        try:
            result = test_func()
            results.append((name, result))
        except Exception as e:
            print(f"\n✗ Test '{name}' failed with exception: {e}")
            import traceback
            traceback.print_exc()
            results.append((name, False))

    # Summary
    print_section("Test Summary")
    passed = sum(1 for _, result in results if result)
    total = len(results)

    for name, result in results:
        status = "✓ PASS" if result else "✗ FAIL"
        print(f"{status}: {name}")

    print(f"\nTotal: {passed}/{total} tests passed")

    if passed == total:
        print("\n🎉 All schema validation tests passed!")
        return 0
    else:
        print(f"\n⚠️  {total - passed} test(s) failed")
        return 1


if __name__ == "__main__":
    sys.exit(main())
```

**Test Results**:
```
======================================================================
  Phase 5: Agent Output Schema Validation Tests
======================================================================
Testing all agent output schemas with sample data...

✓ PASS: Intake Agent
✓ PASS: Coverage Agent
✓ PASS: Fraud Agent
✓ PASS: Severity Agent
✓ PASS: Recommendation Agent
✓ PASS: Explainability Agent
✓ PASS: Missing Fields
✓ PASS: Schema Discovery

Total: 8/8 tests passed

🎉 All schema validation tests passed!
```

**Test Coverage**:
- ✅ All 6 worker agent schemas validated with realistic data
- ✅ Orchestrator output schema tested
- ✅ Invalid data correctly rejected (score out of range)
- ✅ Missing required fields caught and reported
- ✅ Schema discovery utilities working
- ✅ Error messages clear and actionable

---

### 5.7 Key Scalability Patterns Demonstrated

1. ✅ **Schema-Driven Validation**: Pydantic v2 models ensure type safety
2. ✅ **Registry Pattern**: Dynamic schema discovery via AGENT_OUTPUT_SCHEMAS
3. ✅ **Configurable Behavior**: All validation settings via environment variables
4. ✅ **Dual-Layer Logging**: JSONL events + Python logs for comprehensive observability
5. ✅ **Context Engineering**: session_id, agent_id, iteration tracked in all events
6. ✅ **Detailed Error Reporting**: Structured error messages with field paths
7. ✅ **Graceful Degradation**: Validation failures allow retry up to limit
8. ✅ **Version Tracking**: Schema version field for evolution
9. ✅ **Flexible Schema Design**: Dict[str, Any] for orchestrator evidence map
10. ✅ **Custom Validators**: Business rule enforcement via field_validator
11. ✅ **Nested Models**: Complex structures (DataQualityIssue, FraudIndicator, Evidence)
12. ✅ **Enum Constraints**: Type-safe choices (RiskBand, ProcessingTrack, etc.)
13. ✅ **Numeric Constraints**: Range validation (ge, le) for scores/amounts
14. ✅ **Collection Constraints**: min_length for required lists
15. ✅ **Optional Fields**: Flexible data for conditional scenarios
16. ✅ **Strict Mode**: extra="forbid" catches schema drift
17. ✅ **Auto-Injection**: Metadata fields added automatically
18. ✅ **JSON Schema Export**: API documentation support via model_json_schema()
19. ✅ **Partial Validation**: Support for incomplete outputs
20. ✅ **Production-Ready Tests**: Comprehensive test coverage with realistic data

---

### 5.8 Phase 5 Summary

**Files Created**:
```
backend/orchestrator/app/schemas/
├── __init__.py (89 lines)
├── agent_outputs.py (410 lines)
└── validators.py (287 lines)

backend/orchestrator/
└── test_schemas.py (375 lines)
```

**Files Modified**:
```
backend/orchestrator/app/
├── config.py (Added SchemaSettings class, ~25 lines added)
└── services/
    └── agent_react_loop.py (Enhanced _validate_output(), ~120 lines added/modified)
```

**Total Lines Added**: ~1,306 lines of production-ready code

**All 7 Agent Schemas Implemented**:
- ✅ IntakeAgentOutput - Data quality and normalization
- ✅ CoverageAgentOutput - Coverage determination with financial validation
- ✅ FraudAgentOutput - Risk assessment with evidence tracking
- ✅ SeverityAgentOutput - Complexity scoring and resource estimation
- ✅ RecommendationAgentOutput - Action planning with confidence
- ✅ ExplainabilityAgentOutput - Evidence map with decision transparency
- ✅ OrchestratorOutput - Workflow completion with metrics

**Validation Utilities**:
- ✅ validate_agent_output() - Core validation with auto-injection
- ✅ get_schema_for_agent() - Schema class retrieval
- ✅ get_schema_json() - JSON schema export for API docs
- ✅ list_available_schemas() - Schema discovery
- ✅ validate_partial_output() - Graceful handling of incomplete outputs
- ✅ SchemaValidationError - Context-aware exception class

**Configuration Settings**:
- ✅ default_version - Schema version (default: "1.0")
- ✅ strict_validation - Enforce strict validation (default: True)
- ✅ validation_failure_limit - Max failures before error (default: 3)
- ✅ log_validation_sample - Log output on failure (default: True)
- ✅ max_validation_sample_chars - Sample size limit (default: 500)

**Test Results**: 8/8 tests passing
- ✅ All 6 worker agent schemas validated
- ✅ Orchestrator schema validated
- ✅ Invalid data rejected correctly
- ✅ Missing fields caught
- ✅ Schema discovery working

**Completion Criteria Met**:
- ✅ All agent output schemas defined with Pydantic v2
- ✅ Schema validation integrated into AgentReActLoopController
- ✅ Configurable validation behavior via SchemaSettings
- ✅ Detailed error reporting with observability
- ✅ Dual-layer logging (JSONL + Python) implemented
- ✅ Context engineering maintained throughout
- ✅ Comprehensive test coverage with realistic data
- ✅ All tests passing

**Design Decisions**:
1. **Dual-Layer Logging**: Structured JSONL events for replay + Python logs for real-time monitoring
2. **Flexible Evidence Map**: Dict[str, Any] for orchestrator to handle partial/complete evidence maps
3. **Configurable Failure Limits**: Balance between strict validation and graceful retry
4. **Auto-Injection**: Metadata fields (agent_id, timestamp) added automatically
5. **Graceful Degradation**: Validation returns False (not exception) to allow ReAct retry
6. **Truncated Samples**: Configurable output sample logging to prevent log explosion
7. **Strict Mode**: extra="forbid" to catch schema drift early
8. **Version Tracking**: Configurable default version with per-environment override

---

## Next Phases

### Phase 6: Orchestrator API & SSE
- REST endpoints (POST /runs, GET /sessions, etc.)
- Server-Sent Events for live streaming
- Background workflow execution

### Phase 7: Frontend UI
- Next.js with TypeScript + Tailwind
- Run Claim page with live progress
- Session Replay page
- Evidence Map visualization

### Phase 8: Documentation & Polish
- Comprehensive README
- DECISIONS.md (architectural choices)
- Sample data
- End-to-end testing

---

## File Inventory

### Configuration Files
- `.env` - Environment configuration
- `docker-compose.yml` - Multi-service orchestration
- `backend/orchestrator/requirements.txt` - Python dependencies

### Registry Files
- `registries/agent_registry.json` - 7 agents (orchestrator + 6 workers)
- `registries/tool_registry.json` - 6 tools
- `registries/model_profiles.json` - 6 model profiles (OpenAI + Claude)
- `registries/governance_policies.json` - 4 policy categories
- `registries/workflows/claims_triage.json` - Advisory workflow

### Core Services
- `backend/orchestrator/app/config.py` - Configuration system
- `backend/orchestrator/app/services/storage.py` - JSONL/JSON storage
- `backend/orchestrator/app/services/registry_manager.py` - Dynamic discovery
- `backend/orchestrator/app/services/governance_enforcer.py` - Policy enforcement
- `backend/orchestrator/app/services/context_compiler.py` - Context management
- `backend/orchestrator/app/services/agent_react_loop.py` - Worker agent loops
- `backend/orchestrator/app/services/orchestrator_runner.py` - Meta-agent loop

### LLM Integration
- `backend/orchestrator/app/services/llm_client.py` - Multi-provider LLM
- `backend/orchestrator/app/prompts/react_prompts.py` - ReAct prompts
- `backend/orchestrator/app/services/response_parser.py` - JSON parsing

### Tools Gateway (Phase 4)
- `tools/tools_gateway/app/main.py` - FastAPI service (3 endpoints)
- `tools/tools_gateway/app/tools/policy_snapshot.py` - Policy retrieval tool
- `tools/tools_gateway/app/tools/fraud_rules.py` - Fraud detection (7 rules)
- `tools/tools_gateway/app/tools/similarity.py` - Historical claims search
- `tools/tools_gateway/app/tools/schema_validator.py` - Data validation
- `tools/tools_gateway/app/tools/coverage_rules.py` - Coverage determination (5 rules)
- `tools/tools_gateway/app/tools/decision_rules.py` - Decision tree (7 branches)
- `tools/tools_gateway/Dockerfile` - Container configuration
- `tools/tools_gateway/requirements.txt` - Python dependencies
- `backend/orchestrator/app/services/tools_gateway_client.py` - HTTP client with retry

### Agent Output Schemas (Phase 5)
- `backend/orchestrator/app/schemas/__init__.py` - Schema package exports (89 lines)
- `backend/orchestrator/app/schemas/agent_outputs.py` - All 7 agent schemas with Pydantic v2 (410 lines)
- `backend/orchestrator/app/schemas/validators.py` - Validation utilities with detailed error reporting (287 lines)
- `backend/orchestrator/test_schemas.py` - Comprehensive test suite (375 lines)
- `backend/orchestrator/app/config.py` - Enhanced with SchemaSettings (~25 lines added)
- `backend/orchestrator/app/services/agent_react_loop.py` - Enhanced _validate_output() (~120 lines added)

---

**Total Lines of Production Code**: ~6,600+ lines (excluding tests, docs)

**Scalability Patterns Demonstrated**: 30+ production patterns

**Status**: Phases 1-5 Complete (Phase 5: Agent Output Schemas ✅ COMPLETE)

---

## Phase 6: Orchestrator API & SSE

**Goal**: Implement REST API with Server-Sent Events for live workflow streaming.

**Status**: ✅ Complete

**Why Needed**: Enables external clients (frontend) to trigger workflows and monitor execution in real-time.

**Dependencies**: All previous phases (1-5)

**Lines of Code**: ~800 lines

---

### 6.1 API Models

**File**: `backend/orchestrator/app/api/models.py`

**Purpose**: Pydantic models for API requests and responses with OpenAPI documentation.

**Key Models**:

#### 6.1.1 RunWorkflowRequest

```python
class RunWorkflowRequest(BaseModel):
    """Request to run a workflow."""
    
    workflow_id: str = Field(..., description="Workflow ID to execute")
    input_data: Dict[str, Any] = Field(..., description="Input data for workflow")
    session_id: Optional[str] = Field(None, description="Optional session ID")
    options: Optional[Dict[str, Any]] = Field(None, description="Optional execution options")
```

**Demonstrates**: Input validation, API contract definition

#### 6.1.2 RunWorkflowResponse

```python
class RunWorkflowResponse(BaseModel):
    """Response from workflow run request."""
    
    session_id: str = Field(..., description="Unique session ID")
    workflow_id: str = Field(..., description="Workflow being executed")
    status: str = Field(..., description="Initial status: running")
    created_at: str = Field(..., description="ISO 8601 timestamp")
    stream_url: str = Field(..., description="SSE stream URL")
    session_url: str = Field(..., description="Session details URL")
```

**Demonstrates**: RESTful resource design, HATEOAS pattern (URLs in response)

#### 6.1.3 Additional Models

- `SessionSummary`: Paginated session lists
- `SessionDetails`: Complete session with events
- `EvidenceMapResponse`: Evidence map retrieval
- `HealthCheckResponse`: Health status
- `ErrorResponse`: Consistent error format

**Scalability Pattern**: Type-safe API contracts enable client code generation and validation.

---

### 6.2 SSE Broadcaster

**File**: `backend/orchestrator/app/services/sse_broadcaster.py`

**Purpose**: Real-time event streaming via Server-Sent Events with multi-client support.

**Architecture**:

```python
class SSEBroadcaster:
    """
    Server-Sent Events broadcaster for real-time workflow updates.
    
    Scalability patterns:
    - Event buffering (max 100 events per session)
    - Multiple concurrent client connections
    - Reconnection support via last_event_id
    - Automatic cleanup on completion
    """
    
    def __init__(self, max_buffer_size: int = 100):
        self.max_buffer_size = max_buffer_size
        
        # Session-level event queues
        self._session_buffers: Dict[str, deque] = {}
        
        # Active client queues
        self._client_queues: Dict[str, List[asyncio.Queue]] = {}
        
        # Session completion flags
        self._completed_sessions: Dict[str, bool] = {}
```

**Key Methods**:

#### 6.2.1 Subscribe to Events

```python
async def subscribe(
    self,
    session_id: str,
    last_event_id: Optional[str] = None
) -> AsyncGenerator[str, None]:
    """
    Subscribe to session events.
    
    Demonstrates:
    - Async generator pattern for streaming
    - Reconnection with buffered events
    - Client-specific queues
    """
    # Create client queue
    client_queue: asyncio.Queue = asyncio.Queue()
    
    # Register client
    if session_id not in self._client_queues:
        self._client_queues[session_id] = []
    self._client_queues[session_id].append(client_queue)
    
    try:
        # Send buffered events if reconnecting
        if last_event_id and session_id in self._session_buffers:
            for buffered_event in self._session_buffers[session_id]:
                if buffered_event.get("id", "") > last_event_id:
                    yield self._format_sse_event(buffered_event)
        
        # Stream new events
        while True:
            event = await client_queue.get()
            if event is None:  # Completion signal
                break
            yield self._format_sse_event(event)
    finally:
        # Unregister client (cleanup)
        self._client_queues[session_id].remove(client_queue)
```

#### 6.2.2 Broadcast Events

```python
async def broadcast_event(
    self,
    session_id: str,
    event_type: str,
    event_data: Dict,
    event_id: Optional[str] = None
) -> None:
    """
    Broadcast event to all subscribers.
    
    Demonstrates: Fan-out pattern for multiple clients
    """
    # Create event
    event = {
        "id": event_id or self._generate_event_id(),
        "event": event_type,
        "data": event_data,
        "timestamp": datetime.utcnow().isoformat() + "Z"
    }
    
    # Buffer event (for reconnections)
    if session_id not in self._session_buffers:
        self._session_buffers[session_id] = deque(maxlen=self.max_buffer_size)
    self._session_buffers[session_id].append(event)
    
    # Broadcast to all connected clients
    if session_id in self._client_queues:
        for client_queue in self._client_queues[session_id]:
            await client_queue.put(event)
```

**SSE Format**:

```
id: 20241223120000_abc12345
event: workflow_started
data: {"workflow_id": "claims_triage", "session_id": "session_..."}

```

**Scalability Patterns**:
- **Async generators**: Non-blocking event streaming
- **Event buffering**: Handles reconnections (last 100 events per session)
- **Fan-out**: Single event → multiple client queues
- **Graceful cleanup**: Removes disconnected clients automatically

---

### 6.3 Workflow Executor

**File**: `backend/orchestrator/app/services/workflow_executor.py`

**Purpose**: Execute workflows in background with SSE broadcasting.

**Architecture**:

```python
class WorkflowExecutor:
    """
    Execute workflows in background with SSE broadcasting.
    
    Scalability patterns:
    - Background task management
    - Non-blocking execution
    - Event streaming integration
    - Artifact persistence
    """
    
    def __init__(self):
        self.broadcaster = get_broadcaster()
        self.session_writer = get_session_writer()
        self.artifact_writer = get_artifact_writer()
        
        # Track running workflows
        self._running_workflows: Dict[str, asyncio.Task] = {}
```

**Key Methods**:

#### 6.3.1 Execute Workflow

```python
async def execute_workflow(
    self,
    workflow_id: str,
    input_data: Dict[str, Any],
    session_id: Optional[str] = None
) -> str:
    """
    Execute workflow in background.
    
    Demonstrates: Async task creation pattern
    """
    # Generate session ID if not provided
    if not session_id:
        session_id = self._generate_session_id()
    
    # Broadcast workflow start
    await self.broadcaster.broadcast_event(
        session_id=session_id,
        event_type="workflow_started",
        event_data={
            "workflow_id": workflow_id,
            "session_id": session_id,
            "started_at": datetime.utcnow().isoformat() + "Z"
        }
    )
    
    # Create background task
    task = asyncio.create_task(
        self._run_workflow_task(session_id, workflow_id, input_data)
    )
    
    self._running_workflows[session_id] = task
    
    return session_id
```

#### 6.3.2 Background Task Runner

```python
async def _run_workflow_task(
    self,
    session_id: str,
    workflow_id: str,
    input_data: Dict[str, Any]
) -> None:
    """
    Background task to run workflow.
    
    Demonstrates: Sync-to-async bridging (orchestrator is sync)
    """
    try:
        # Create orchestrator runner
        orchestrator = create_orchestrator_runner(
            session_id=session_id,
            workflow_id=workflow_id
        )
        
        # Execute workflow (sync call via executor)
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None,  # Use default executor
            orchestrator.execute,
            input_data
        )
        
        # Broadcast completion
        await self.broadcaster.broadcast_event(
            session_id=session_id,
            event_type="workflow_completed",
            event_data={
                "status": result.status,
                "completion_reason": result.completion_reason,
                "agents_executed": result.agents_executed
            }
        )
        
        # Save evidence map as artifact
        if result.evidence_map:
            artifact_id = f"{session_id}_evidence_map"
            self.artifact_writer.write_artifact(
                artifact_id=artifact_id,
                artifact_type="evidence_map",
                data=result.evidence_map
            )
        
        # Complete session
        await self.broadcaster.complete_session(session_id)
        
    except Exception as e:
        logger.error(f"Workflow execution failed: {e}")
        
        # Broadcast error
        await self.broadcaster.broadcast_event(
            session_id=session_id,
            event_type="workflow_error",
            event_data={"error": str(e)}
        )
        
        await self.broadcaster.complete_session(session_id)
    
    finally:
        # Cleanup
        del self._running_workflows[session_id]
```

**Scalability Patterns**:
- **Background execution**: Non-blocking workflow runs
- **Sync-to-async bridge**: `run_in_executor` for sync OrchestratorRunner
- **Event broadcasting**: Real-time progress updates
- **Graceful error handling**: Errors don't crash the executor
- **Cancellation support**: Can cancel running workflows

---

### 6.4 Runs API Endpoints

**File**: `backend/orchestrator/app/api/runs.py`

**Purpose**: Workflow execution and SSE streaming endpoints.

**Endpoints**:

#### 6.4.1 POST /runs - Create Workflow Run

```python
@router.post("", response_model=RunWorkflowResponse)
async def create_run(request: RunWorkflowRequest):
    """
    Create new workflow run.
    
    Demonstrates:
    - Async request handling
    - Validation before execution
    - Background task creation
    """
    # Validate workflow exists
    registry = get_registry_manager()
    workflow = registry.get_workflow(request.workflow_id)
    
    if not workflow:
        raise HTTPException(status_code=404, detail="Workflow not found")
    
    # Execute workflow in background
    executor = get_workflow_executor()
    session_id = await executor.execute_workflow(
        workflow_id=request.workflow_id,
        input_data=request.input_data,
        session_id=request.session_id
    )
    
    # Return response with URLs
    return RunWorkflowResponse(
        session_id=session_id,
        workflow_id=request.workflow_id,
        status="running",
        created_at=datetime.utcnow().isoformat() + "Z",
        stream_url=f"/runs/{session_id}/stream",
        session_url=f"/sessions/{session_id}"
    )
```

#### 6.4.2 GET /runs/{session_id}/stream - SSE Stream

```python
@router.get("/{session_id}/stream")
async def stream_run(
    session_id: str,
    request: Request,
    last_event_id: Optional[str] = None
):
    """
    Stream workflow execution events via SSE.
    
    Demonstrates:
    - SSE streaming
    - Reconnection support
    - Client disconnection detection
    """
    broadcaster = get_broadcaster()
    
    async def event_generator():
        async for event in broadcaster.subscribe(session_id, last_event_id):
            # Check if client disconnected
            if await request.is_disconnected():
                break
            yield event
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # Disable nginx buffering
        }
    )
```

#### 6.4.3 Additional Endpoints

- `GET /runs/{session_id}/status` - Quick status check
- `POST /runs/{session_id}/cancel` - Cancel workflow
- `GET /runs` - List running workflows

**Scalability Patterns**:
- **Async endpoints**: Non-blocking request handling
- **Streaming responses**: SSE for real-time updates
- **HATEOAS**: Resource URLs in responses
- **Client detection**: Graceful handling of disconnections

---

### 6.5 Sessions API Endpoints

**File**: `backend/orchestrator/app/api/sessions.py`

**Purpose**: Session replay and evidence map retrieval.

**Endpoints**:

#### 6.5.1 GET /sessions - List Sessions (Paginated)

```python
@router.get("", response_model=List[SessionSummary])
async def list_sessions(
    limit: int = Query(default=20, ge=1, le=100),
    offset: int = Query(default=0, ge=0)
):
    """
    List recent sessions.
    
    Demonstrates: Pagination pattern
    """
    reader = get_session_reader()
    
    # Get all session files
    storage_path = Path("/storage/sessions")
    session_files = sorted(
        storage_path.glob("*.jsonl"),
        key=lambda f: f.stat().st_mtime,
        reverse=True
    )
    
    # Apply pagination
    session_files = session_files[offset:offset + limit]
    
    # Build summaries
    summaries = []
    for session_file in session_files:
        session_id = session_file.stem
        events = reader.read_session(session_id)
        
        # Extract summary info
        summary = SessionSummary(
            session_id=session_id,
            workflow_id=events[0].get("event_data", {}).get("workflow_id"),
            status=events[-1].get("event_type"),
            created_at=events[0].get("timestamp"),
            event_count=len(events),
            agents_executed=[...]  # Extracted from events
        )
        summaries.append(summary)
    
    return summaries
```

#### 6.5.2 GET /sessions/{session_id} - Session Details

```python
@router.get("/{session_id}", response_model=SessionDetails)
async def get_session(
    session_id: str,
    event_type: Optional[str] = Query(None, description="Filter events by type")
):
    """
    Get complete session details with events.
    
    Demonstrates: Event filtering
    """
    reader = get_session_reader()
    
    # Read events (with optional filter)
    if event_type:
        events = reader.filter_events(session_id, event_type)
    else:
        events = reader.read_session(session_id)
    
    if not events:
        raise HTTPException(status_code=404, detail="Session not found")
    
    # Build details from events
    details = SessionDetails(
        session_id=session_id,
        workflow_id=...,
        status=...,
        events=events,
        ...
    )
    
    return details
```

#### 6.5.3 GET /sessions/{session_id}/evidence - Evidence Map

```python
@router.get("/{session_id}/evidence", response_model=EvidenceMapResponse)
async def get_evidence_map(session_id: str):
    """
    Get evidence map for session.
    
    Demonstrates: Artifact retrieval
    """
    artifact_reader = get_artifact_reader()
    artifact_id = f"{session_id}_evidence_map"
    
    artifact = artifact_reader.read_artifact(artifact_id)
    
    return EvidenceMapResponse(
        session_id=session_id,
        evidence_map=artifact["data"],
        generated_at=artifact["created_at"]
    )
```

#### 6.5.4 Additional Endpoints

- `GET /sessions/{session_id}/events/{event_type}` - Filtered events
- `DELETE /sessions/{session_id}` - Delete session

**Scalability Patterns**:
- **Pagination**: Prevents large response payloads
- **Event filtering**: Selective data retrieval
- **Filesystem-based**: Easy to query without database
- **RESTful design**: Standard HTTP methods

---

### 6.6 Main FastAPI Application

**File**: `backend/orchestrator/app/main.py`

**Purpose**: Complete FastAPI application with routers, middleware, and lifecycle management.

**Architecture**:

```python
# Create FastAPI app
app = FastAPI(
    title="AgentMesh Orchestrator",
    description="Production-scale multi-agent orchestration platform",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3016"],  # Frontend URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global exception handler
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """Consistent error response format."""
    return JSONResponse(
        status_code=500,
        content=ErrorResponse(
            detail=str(exc),
            error_type=type(exc).__name__,
            timestamp=datetime.utcnow().isoformat() + "Z"
        ).model_dump()
    )

# Include routers
app.include_router(runs.router)
app.include_router(sessions.router)
```

**Lifecycle Management**:

#### 6.6.1 Startup

```python
@app.on_event("startup")
async def startup():
    """
    Initialize on startup.
    
    Demonstrates: Dependency initialization
    """
    logger.info("Starting AgentMesh Orchestrator")
    
    # Load configuration
    config = get_config()
    logger.info(f"Storage path: {config.storage_path}")
    
    # Initialize registries
    init_registry_manager()
    
    registry = get_registry_manager()
    stats = registry.get_stats()
    
    logger.info(f"Registries loaded:")
    logger.info(f"  - Agents: {stats['agents_count']}")
    logger.info(f"  - Tools: {stats['tools_count']}")
    logger.info(f"  - Models: {stats['models_count']}")
    logger.info(f"  - Workflows: {stats['workflows_count']}")
    
    logger.info("Orchestrator ready - API available at /docs")
```

#### 6.6.2 Shutdown

```python
@app.on_event("shutdown")
async def shutdown():
    """
    Cleanup on shutdown.
    
    Demonstrates: Graceful shutdown
    """
    logger.info("Shutting down Orchestrator")
    
    # Cancel any running workflows
    executor = get_workflow_executor()
    running_sessions = executor.get_running_sessions()
    
    if running_sessions:
        logger.info(f"Cancelling {len(running_sessions)} running workflows")
        for session_id in running_sessions:
            await executor.cancel_workflow(session_id)
    
    logger.info("Shutdown complete")
```

**Core Endpoints**:

#### 6.6.3 Health Check

```python
@app.get("/health", response_model=HealthCheckResponse)
async def health_check():
    """
    Health check endpoint.
    
    Demonstrates: System health monitoring
    """
    registry = get_registry_manager()
    registries_loaded = True
    
    return HealthCheckResponse(
        status="healthy",
        timestamp=datetime.utcnow().isoformat() + "Z",
        version="1.0.0",
        registries_loaded=registries_loaded
    )
```

#### 6.6.4 System Stats

```python
@app.get("/stats")
async def get_stats():
    """
    Get system statistics.
    
    Demonstrates: Observability endpoint
    """
    registry = get_registry_manager()
    executor = get_workflow_executor()
    broadcaster = get_broadcaster()
    
    return {
        "registries": registry.get_stats(),
        "executor": executor.get_stats(),
        "broadcaster": broadcaster.get_stats(),
        "timestamp": datetime.utcnow().isoformat() + "Z"
    }
```

**Scalability Patterns**:
- **Router composition**: Modular endpoint organization
- **CORS middleware**: Frontend integration ready
- **Global exception handler**: Consistent error responses
- **Lifecycle hooks**: Proper initialization and cleanup
- **Health checks**: Kubernetes/Docker-ready
- **Observability**: Stats endpoint for monitoring

---

### 6.7 Dockerfile

**File**: `backend/orchestrator/Dockerfile`

**Purpose**: Container configuration for orchestrator service.

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*

# Copy requirements and install
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY app/ ./app/

# Create storage directories
RUN mkdir -p /storage/sessions /storage/artifacts /storage/compactions

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Run uvicorn
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Scalability Patterns**:
- **Multi-stage potential**: Can add build stage for optimization
- **Health checks**: Docker/Kubernetes ready
- **Minimal base**: python:3.11-slim for small image size
- **Non-root user**: (Can be added for production security)

---

### 6.8 Phase 6 Completion Summary

**Files Created** (7 files, ~800 lines):

1. `backend/orchestrator/app/api/__init__.py` - Package initialization
2. `backend/orchestrator/app/api/models.py` - Request/response schemas (130 lines)
3. `backend/orchestrator/app/services/sse_broadcaster.py` - SSE broadcaster (230 lines)
4. `backend/orchestrator/app/services/workflow_executor.py` - Background executor (180 lines)
5. `backend/orchestrator/app/api/runs.py` - Workflow execution endpoints (160 lines)
6. `backend/orchestrator/app/api/sessions.py` - Session replay endpoints (220 lines)
7. `backend/orchestrator/app/main.py` - Main FastAPI application (180 lines)
8. `backend/orchestrator/Dockerfile` - Container configuration (25 lines)
9. `backend/orchestrator/app/__init__.py` - App package initialization

**Integration Points**:

✅ **SSE Broadcaster** → Event streaming
✅ **Workflow Executor** → OrchestratorRunner integration
✅ **Runs API** → WorkflowExecutor → SSE broadcasting
✅ **Sessions API** → Storage layer (JSONL/JSON)
✅ **Main app** → Registry initialization
✅ **Main app** → Router composition
✅ **CORS** → Frontend integration ready

**API Endpoints Summary**:

**Runs** (5 endpoints):
- `POST /runs` - Create workflow run
- `GET /runs/{session_id}/stream` - SSE event stream
- `GET /runs/{session_id}/status` - Quick status
- `POST /runs/{session_id}/cancel` - Cancel workflow
- `GET /runs` - List running workflows

**Sessions** (5 endpoints):
- `GET /sessions` - List sessions (paginated)
- `GET /sessions/{session_id}` - Session details
- `GET /sessions/{session_id}/evidence` - Evidence map
- `GET /sessions/{session_id}/events/{event_type}` - Filtered events
- `DELETE /sessions/{session_id}` - Delete session

**System** (3 endpoints):
- `GET /` - Root endpoint
- `GET /health` - Health check
- `GET /stats` - System statistics

**Total**: 13 API endpoints

**Scalability Patterns Demonstrated**:

1. **Async API**: Non-blocking request handling
2. **SSE Streaming**: Real-time event delivery
3. **Event Buffering**: Reconnection support
4. **Fan-out Broadcasting**: Multiple client support
5. **Background Tasks**: Non-blocking workflow execution
6. **Sync-to-Async Bridge**: `run_in_executor` pattern
7. **Router Composition**: Modular API design
8. **HATEOAS**: Resource URLs in responses
9. **Pagination**: Scalable list operations
10. **Health Checks**: Kubernetes-ready
11. **Graceful Shutdown**: Workflow cancellation
12. **Global Exception Handler**: Consistent errors
13. **CORS Middleware**: Frontend integration
14. **Observability**: Stats endpoints

**Completion Criteria Met**:

- ✅ API models with Pydantic validation
- ✅ SSE broadcaster with reconnection support
- ✅ Background workflow executor
- ✅ Runs endpoints (create, stream, status, cancel)
- ✅ Sessions endpoints (list, details, evidence)
- ✅ Main FastAPI application
- ✅ CORS configuration for frontend
- ✅ Lifecycle management (startup/shutdown)
- ✅ Health checks
- ✅ All files compile without errors
- ✅ Dockerfile created

**Design Decisions**:

1. **SSE over WebSockets**: Simpler protocol, auto-reconnection, unidirectional (sufficient for our use case)
2. **Event Buffering (100 events)**: Balance between memory and reconnection capability
3. **Background asyncio Tasks**: Non-blocking workflow execution
4. **Singleton Patterns**: Broadcaster and Executor for shared state
5. **Sync-to-Async Bridge**: OrchestratorRunner is sync, API is async - bridge via `run_in_executor`
6. **JSONL for Sessions**: Already using for events, easy to stream and query
7. **Flat-File Artifacts**: Simplifies Phase 6, production would use S3/blob storage
8. **Pagination Default (20)**: Prevents large responses
9. **Global Exception Handler**: Consistent error format for all endpoints
10. **CORS Middleware**: Explicit frontend URL for security

---

**Status**: Phase 6 Complete ✅

**Next Phase**: Phase 7 - Frontend UI (Next.js with live progress streaming)

